

Digitalisierung

Sie möchten Ihr Unternehmen digitalisieren? Super, die richtige Entscheidung um sich in Zukunft gegen die Konkurrenz zur Wehr zu setzen. Aber es wird eine lange beschwerliche Reise bei dem Sie Mut und Fehlertoleranz benötigen und sich auch unangenehmen Wahrheiten stellen müssen. Die Digitalisierung wird Ihnen vor allem eines bringen: **Transparenz**

Transparenz über Ihr Unternehmen in einem nie gekannten Ausmaß mit der Möglichkeit die richtigen Entscheidungen oder richtige Strategie auf fundierten Fakten abzuleiten, aber auch neue Business Modelle die das Überleben Ihres Unternehmens sichern können. Vielleicht werden Sie feststellen, dass Ihre Organisation mit der Geschwindigkeit der neuen technologischen Möglichkeiten nicht zurechtkommt, dass festgefahrene Prozesse Ihnen den Umstieg schwer machen oder dass Sie oder Ihre Mitarbeiter sich mit dem „New Work“ nicht abfinden können. Trotzdem ist der Weg der Richtige und muss gegangen werden, sonst tun es andere und sie bleiben zurück.

In der Digitalisierung versucht man aktuelle Prozesse und Tätigkeiten die noch von Menschen erledigt werden, mit einem Computer zu automatisieren, z.B. das Erledigen von Steuererklärungen oder das Fahren von Autos. Im Prinzip geht es darum alle Tätigkeiten, die man automatisieren kann, auch zu automatisieren. Außerdem man möchte mit vielen Daten neue Einsichten in aktuelle Prozesse bekommen um die Prozesse zu verbessern.

Leider kommt es oft vor, dass Unternehmen technisches Wissen fehlt, um eine Digitalisierung sinnvoll vorwärts zu treiben. Unrealistische Ziele und Budgets, unzureichend ausgebildete Mitarbeiter und Probleme beim Finden und Einstellen von guten qualifizierten Mitarbeitern, dass die Digitalisierung nur schleppend voran geht oder auch für viel Geld kein größeres Ergebnis zu sehen ist. 

Unternehmen die diesen Weg gehen wollen, machen sich auf eine Reise in ein Gebiet in denen andere Regeln gelten. Neue andersartige Prozesse, Mitarbeiter die es gewohnt sind eigene Entscheidung zu treffen, ein komplett neues Mindset gegenüber Fehlern und Weiterentwicklung und eine nie da gewesen Geschwindigkeit in der sich Technologien ändern. Sie treffen auf einen Arbeitnehmermarkt wo IT-Experten rar sind. Das alles überfordern aktuelle Organisationen, Prozesse und die Menschen in den Abteilungen. Dieses Buch soll dazu dienen einen Grundstock an Wissen über technische IT und Anwendungsentwicklung zu vermitteln

Es ist für Manager und Mitarbeiter von mittelständischen und größeren Unternehmen gedacht, die in einer Digitalisierungsabteilung arbeiten ohne Vorkenntnisse und die eine eigene Anwendungsentwicklung in Betracht ziehen. Entweder durch ein eigenes Entwicklungsteam oder durch externe Softwareentwicklungsdienstleister:

- Projektmanager, die die Aufgabe haben Prozesse in einem Unternehmen zu digitalisieren.
- Product Owner der mehr darüber erfahren wollen, worüber Software Entwickler so sprechen
- Quereinsteiger in die Digitalisierung. Sie haben spezifisches fachliches Know How und Ihre Expertise wird in einem Digitalisierungsprojekt benötigt.
- Agile Coaches / Scrum Master die keinen Informatikhintergrund haben.
- Team Leader oder höheres Management die Ihre Abteilungen digitalisieren möchten und zumindest die „Sprache“ der Entwickler verstehen wollen

Dieses Buch hat eine relativ hohe Informationsdichte. Die Kapitel gehen schnell, aber auch so tief in die Themen hinein, wie der Autor für notwendig hält um Sie für das Thema fit zu machen. Dieses Buch ist nicht dazu da um Sie zum vollwertigen IT-Experten oder Software Entwickler auszubilden, sondern dass Sie einen Überblick bekommen. Es wird empfohlen sich Kapitel für Kapitel vorzunehmen und nebenbei mit einer Suchmaschine im Internet die Themen weiter zu vertiefen. Es kann vorkommen, dass Sie in den Kapiteln „verloren“ gehen. Lassen Sie sich nicht entmutigen, IT ist ein sehr komplexes abstraktes Thema. Sollten Sie das Gefühl haben, dass Sie ein Kapitel nicht verstanden haben, können Sie das Internet zu Rate ziehen. Aber geben Sie nicht auf es lohnt sich auf alle Fälle sich in dem Thema fortzubilden. In dem Buch werden manche Themen mit dem Icon  markiert und dahinter die den entsprechenden Suchbegriff. Damit können Sie auf einer Suchmaschine noch nähere Informationen finden. 

Wir steigen direkt ein in die technischen Themen mit dem Versuch diese aufeinander aufzubauen. Das ist das Hauptthema dieses Buches. Danach wird das Thema Projektmanagement von Software Entwicklung beleuchtet. Abschließend wird das Thema wie man eine Idee zur Umsetzung bringt besprochen. 

Technische Grundbegriffe


Software


IDE

Um Software zu entwickeln, gibt es extra Apps für Software Entwickler, die Integrated Developement Environments kurz IDEs. Was die Textverarbeitung für den Schriftsteller ist, ist die IDE für den Entwickler. Es gibt eine Reihe guter IDEs. Bekannt und frei verfügbar ist z.B. Visual Studio Code. IDEs zeichnen sich dadurch aus, dass sie den Entwickler in vielerlei Hinsicht unterstützten. Auf der linken Seite hat man normalerweise eine Übersicht über die Dateien des Projekts. Rechts hat man den Editor, in dem man den Code schreibt. Der Editor unterstützt einen mit vielen Features:

- **Syntax Highlighting**, der Code wird in verschiedenen Farben dargestellt, um ihn besser lesen zu können. Außerdem zeigt er einem schon während man programmiert Syntaxfehler an, z.B. wenn man eine Klammer vergessen hat. 
- **Intellisense,** der Editor zeigt Vorschläge an, während man tippt, was man als nächstes eingeben könnte. Das kann man sich wie die automatischen Vorschläge am Handy vorstellen, während man Nachrichten tippt. Wenn man mit unzähligen Klassen und Variablen zu tun hat dann ist das sehr hilfreich, wenn man einen Buchstaben eingibt und man bekommt die Vorschläge welche Variablen mit dem Buchstaben beginnen.
- **Debuggen,** nachdem Softwareprogramme sehr komplex werden können, gibt es die Möglichkeit, das Programm zu starten, sich mit einem Debugger anzuhängen. Dieser Debugger ist normalerweise in der IDE integriert. Damit kann man das Programm mit Breakpoints unterbrechen, kann den Code Schritt für Schritt durchgehen und sich den Inhalt der Variablen genau ansehen. Das ist sehr hilfreich Fehler zu finden.
1. 

Die IDE integriert wohl sehr viele Themen, die wir unten noch betrachten. Das Versionskontrollsystem, die Testautomatisierung usw.

1. 

Algorithmus

Hat man jetzt die Programmiersprache gelernt und im Idealfall eine IDE installiert kommt jetzt der schwierige Teil. Man möchte ein Ziel erreichen also ein Feature umsetzen. Der Code, den man schreiben muss, um das Problem zu lösen, das muss der Entwickler sich ausdenken. Das ist ein kreativer Prozess und gar nicht so einfach. Man hat ein Problem, wie löst man es mit den Mitteln, die einem eine Programmiersprache und damit ja ein Computer bietet. Muss man z.B. eine Liste von Strings, also Zeichenfolgen, sortieren, wie schafft man das mit den Dingen, die Sie jetzt kennen gelernt haben. Wenn Sie eine Funktion schreiben müssten ,wie würden es anstellen, dass diese Liste aus Zeichenfolgen die übergeben wird sortiert zurückgegeben wird. Man muss dabei beachten, dass sie den Listeninhalt nicht kennen, die aktuelle Sortierung und die Länge wird sich jedes Mal unterscheiden und natürlich kann es auch sein dass die Liste sehr groß ist? 

1. 
1. 
1. **function sortList(liste) {**
1. 	//Hier muss der Code hin der die Liste sortiert
1. 	…
1. 
1. 	**return sortedList;**
1. **}	
1. 
1. **let l = [„q“,“w“,“e“,“r“,“t“,“z“,“u“,“i“,“o“,“p“,“l“];**
1. 
1. 
1. 
1. **let mySortedList = sortList(l);**
1. 
1. //hier sollte folgendes ausgegeben warden:
1. //e, i, l, o, p, q, r, t, u, w, z 
1. 
1. **console.log(mySortedList)**
1. 

Es lohnt sich auf alle Fälle einmal zu versuchen sich einen Sortieralgorithmus auszudenken. Tatsächlich ist das nicht besonders einfach. 

Also, ein Algorithmus ist ein Vorgehen, welches man immer wieder wiederholen kann und das ein Problem löst. Und eine Funktion ist somit ein Algorithmus. Dafür gibt es ganze Bücher, die sich mit einer Reihe von schon fertigen Algorithmen in der Software beschäftigen. Außerdem gibt es eine Vielzahl von Libraries die den Entwickler Arbeiten abnehmen. Auch das Problem der Sortierung ist schon gelöst. Es gibt dafür viele unterschiedliche Algorithmen wie BubbleSort, SelectionSort, QuickSort. ( Sortieralgorithmen)

Aber das macht auch den Spaß an Programmierung aus, Probleme lösen. Das kann sehr herausfordernd sein und auch sehr viel Spaß machen wenn man es schafft das Problem zu lösen. Wenn Sie das ausprobieren möchten, es gibt gute Internetseiten, bei denen man Algorithmen entwickeln kann wie Hackerrank oder Coding Game.

Testautomatisierung

Hat man jetzt einen Algorithmus entwickelt und möchte sicherstellen dass dieser auch dauerhaft richtig läuft gibt es zwei Möglichkeiten. Man probiert es manuell oder man Automatisiert den Test. Die Testautomatisierung ist ein wichtiger Bestandteil der modernen Softwareentwicklung. Bis Anfang der 2010er Jahre hat man die Testautomatisierung sehr stark vernachlässigt. Die Software wurde manuell von Menschen getestet oft auch von dem Entwickler selbst. Manuelle Tests sind zeitaufwendig und sind vor allem sehr fehleranfällig. Wenn man alle Funktionen mit vielen möglich Kombinationen schon bei einer kleinen Webseite testen möchte, ist das eine Herausforderung. Man darf nicht nur sinnvolle Eingaben testen. Die Benutzer geben nicht immer nur sinnvolle Daten ein. Also umfasst der Test auch die Validierungen der eingegebenen Daten, was passiert wenn man Daten weglässt, wie schnell reagiert das Programm wenn man große Datenmengen eingegeben hat usw. 

Hier kommen wir zu einem Problem was jede Software über kurz oder lang scheitern lässt, wenn man die Tests nicht automatisiert. Bei jedem Release wird Code geändert neue Features hinzugefügt. Das kann aber dazu führen, dass schon getesteten Features nicht mehr funktionieren. D.h. man muss bei jedem Release nicht nur die neuen Features testen, sondern auch die schon vorhandenen Features. Das führt am Ende dazu, dass die Testzeit die vor jedem Release investiert wird immer länger und länger wird. Dann hat man zwei Möglichkeiten, entweder man lässt die Qualität sinken, indem man weniger testet oder man reduziert die Anzahl der neuen Features. Über kurz oder lang wird es das Projekt scheitern lassen. Irgendwann haben Sie ein Legacysystem geschaffen, welches sehr stark in Ihrem Unternehmen verwurzelt ist aber keiner traut sich noch einmal Änderungen vorzunehmen. Hier hilft nur, Geld in die Hand nehmen und neu entwickeln. Das ist kein Horrorszenario, das ist die Software Entwicklung bis in die 2000er Jahre hinein und ist es heute auch noch, wenn man nicht erkennt wie wichtig Testautomatisierung ist. 

Ein Legacy System ist eine Software die man als Altlast bezeichnen kann. Eine Software die in dem Unternehmen verwachsen, aber unwartbar geworden ist. Man möchte diese Software eigentlich neu entwickeln, weil sie an Ihre technischen Grenzen stößt, aber die Software ist so stark in das Unternehmen integriert mit sehr vielen Sonderfällen die nicht dokumentiert sind, dass es viel Geld kostet diese abzulösen.

In der Testautomatisierung wird nicht nur das Feature entwickelt, sondern auch Code, der testet ob das Feature auch funktioniert. Nehmen wir das Beispiel von oben, also ein Funktion die eine Liste sortiert, wird entwickelt. Der Entwickler implementiert den Algorithmus, dann schreibt man zusätzlich eine Testfunktion die den Algorithmus aufrufen und überprüfen ob die Liste sortiert zurückkommt. Im Idealfall schreibt man sogar mehrere Testfunktionen, die alle Testfälle abdecken. Kann man der Sortierfunktion z.B. mitgeben, dass die Liste aufsteigend oder absteigend sortiert wird, hat man schon zwei Testfälle. Man kann eine große Liste mitgeben, um zu prüfen, ob der Algorithmus in einer bestimmten Zeit durchläuft. Also man entwickelt unter Umständen sehr viel Testcode. 

Ein großes Problem bei Testautomatisierung ist, dass der produktive Code testbar programmiert werden muss. Wenn man hier die Tests nicht sofort zu Beginn eines Projektes mit entwickelt wird es schwer und teuer diese nachzuziehen. Deswegen kann man, wenn man ein Legacyprojekt hat, wo die Testphase immer länger wird, nicht einfach die Testautomatisierung nachziehen. Hier hilft nur eine Neuimplementierung.

Gerade in agilen Projekten, wo sich ständig die Anforderungen ändern und man in kurzen Zyklen arbeitet, ist eine Testautomatisierung essenziell. Wenn man in zwei Wochen Sprints Features entwickeln und diese auch Testen muss, sind manuelle Tests kontraproduktiv. Nichts destotrotz sind manuelle Testen nötig und sinnvoll.

Es gibt mehrere Ebenen für Tests und auch sehr unterschiedliche Arten von Tests, dass es ganze Bücher füllt. Hier die wichtigsten:

**Unit Tests**: Testet eine Funktion, einen Algorithmus, die kleinste Einheit im Code. Man ruft eine Funktion mit verschiedenen Übergabeparameter auf und überprüft, ob das Ergebnis stimmt. Die Übergabeparameter müssen variieren, um verschiedene Anforderungen an die Funktion zu testen

**Integration Tests**: Die Integration Tests testen ein Zusammenspiel von mehreren Funktionen. Z.B. könnte man eine API eines Programms aufrufen und testen, ob die Daten die zurückkommen auch stimmen. 

**UI Tests**: UI Tests sind Tests die die Maus bewegen und das Klicken des Nutzers nachahmen. Die Tests dauern normalerweise auch so lange wie ein echter Mensch benötigen würde. Das sollte man nicht übertreiben und vielleicht die Hauptfeatures testen. Das kann schnell ausufern und nichts ändert sich so schnell wie die UI und am Ende ist man nur noch am Nachziehen der Tests. Auch wenn es dauert bis die Tests durchlaufen kann man diese viel leichter parallelisieren als wenn man noch mehrere Tester einstellt.

**Smoke Test**: Ein einfacher Test, der überprüft ob das System überhaupt noch hochfährt

**Manuelle Tests**: Nichtsdestotrotz sollte ein Mensch nochmal über die Anwendung drüber sehen und die Haupt-UseCases zu testen.

Versionierung

Um Code zu verwalten haben Entwickler ein eigenes Programm entwickelt, ein Versionskontrollsystem. Das bekannteste ist Git, welches frei verfügbar ist. In einem Versionskontrollsystem (VCS) kann man seinen aktuellen Stand zwischenspeichern und einen Kommentar hinterlegen, in dem der Entwickler erklärt was man im Code geändert hat. Dadurch erhält man eine Historie von allen Änderungen und kann sogar auf einen alten Stand zurückspringen. Dazu muss man das VCS in dem Ordner initialisieren, in dem der Code liegt. Diesen Ordner nennt man dann ein Repository. Der Entwickler kann dann ganz einfach seinen Code ändern und in das Versionskontrollsystem speichern. Das nennt man einen checkin oder auch einen commit. Dazu wird gespeichert wer welchen Code wie geändert hat.


image016.png (Abbildung 8 Mehrere Checkins in das VCS)

Man kann seinen Code auch verzweigen auch branchen genannt. Mit einem Branch erstellt man eine komplette Kopie des aktuellen Standes. Wenn man den Branch auswählt, kann man darauf arbeiten ohne den originalen Stand des Codes zu verändern. Das verwendet der Entwickler z.B. wenn er ein neues Feature entwickelt oder auch nur etwas ausprobieren möchte. Wenn der Entwickler mit seiner Arbeit zufrieden ist spielt man die Änderungen auf den das Original, das nennt man mergen. Man kann zwischen zwei Branches hin und herspringen. Das VCS sorgt dafür, dass man auf der entsprechenden Version der Dateien arbeitet. 

Ein Entwickler möchte ein neues Feature entwickeln:

- Er arbeitet auf dem Originalstand des Codes oft auch master branch genannt.
- Er erstellt einen neuen Branch mit dem Namen des Features z.B. sort
- Er entwickelt das neue Feature, checkt in den Branch neue Änderungen ein
- Inzwischen ist ein Bug in dem aktuellen Release aufgetreten der gefixed werden muss, also wechselt der Entwickler zu dem master branch. Das VCS stellt den master branch wieder her. Der Entwickler fixed den Bug, checked ihn ein und stellt einen Patch bereit. 
- Danach wechselt der Entwickler wieder zu seinem Branch sort wo seine vorher eingecheckten Änderungen wieder hergestellt werden.
- Den Patch aus dem master branch kann er mit einem rebase in den Branch sort übernehmen
- Wenn das Feature umgesetzt ist, kann man die Änderungen in den master branch mit einem merge übernehmen und den Branch sort löschen. 

image017.png (Abbildung 9 Branchen eines Codes und mergen in den originalen Branch)

Das Versionskontrollsystem ermöglicht es vor allem, dass mehrere Entwickler zusammenarbeiten können auf einer Codebasis. Dazu wird ein Repository auf einem zentralen Server eingerichtet und die Entwickler Klonen das Repository auf ihren PC. Das ist ein einmaliger Vorgang. Beim Klonen wird das Repository lokal eingerichtet und mit dem Repository auf dem Server verbunden. Die größte Sammlung von Opensource Projekten ist auf den Servern von Github zu finden.

image018.png (Abbildung 10 Pull von einem zentralen Repository)

Das initiale Herunterladen von einem Repository nennt man klonen. Wenn man Änderungen von einem anderen Entwickler auf seinem Klon integrieren will nennt man das pull. Und wenn man lokale Änderungen in das zentrale Repository zurückspielen will, nennt man das einen push.


Abbildung 11 Push und Pull mit Server

Nachdem Entwickler lokal auf ihrem aktuellen Stand arbeiten und erst bei einem Pull die neue Version bekommen, kommt es vor, dass mehrere Entwickler lokal bei sich die gleichen Dateien ändern und diese wieder auf den Server zurückspielen. Das Versionkontrollsystem versucht die Änderungen der Dateien automatisch zusammenzuführen, also zu mergen. Das funktioniert normalerweise ganz gut, allerdings kann es auch zu Konflikten führen. D.h. dass Versionskontrollsystem kann nicht entscheiden wie die Änderungen zusammenzuführen sind und meldet das dem Entwickler. Der muss dann die Mergekonflikte manuell auflösen. 

Abbildung 12 Mergekonflikt

Wie in dem Bild dargestellt ändern zwei Entwickler lokal die gleiche Datei und wollen diese auf das zentrale Repository pushen. Das funktioniert tatsächlich solange sie nicht die gleichen Zeilen ändern. Aber in dem Beispiel wurde ein und dieselbe Zeile geändert. Das VCS löst einen Mergekonflikt aus und derjenige der als zweites die Version pushen wollte muss den Konflikt bei sich lokal lösen. Das kann viel Zeit kosten und ein Projekt verzögern. Ständige Mergekonflikte können auf vielfältige Probleme im Projekt hinweisen.

Wenn Mergekonflikte häufiger auftreten hat man mehrere Möglichkeiten:

- die Entwickler können kleinere Codeänderungen einchecken, dann fällt es dem Mergealgorithmus leichter Dateien zusammenzuführen
- die Architektur könnte kleinteiliger werden, d.h. mehr und kleinere Dateien, dann arbeiten Entwickler weniger in den gleichen Dateien.
- die Priorisierung der Aufgaben ändern, so dass Entwickler in verschiedenen Bereichen der Anwendung arbeiten und nicht gleiche Dateien ändern
- die Aufgaben der Entwickler können kleiner geschnitten werden, was zu kleineren Codeänderungen führt.

Wie auch immer Sie das Problem lösen, viele Mergekonflikte sind ein häufiges Problem, dass einem Projekt unnötig Zeit kostet und das vermieden werden sollte. 

Wenn Code geändert wurde, sollte von anderen Entwicklern der geänderte Code überprüft werden. Das nennt man ein Code Review. Dabei können die Entwickler Kommentare hinterlassen und darüber diskutieren, ob es einen besseren Weg gibt, den Code verändern und am Schluss, wenn alle zufrieden sind den Code einchecken. Das macht man oft vor dem Mergen des Codes und wird Merge Request oder Pull Request genannt. Das funktioniert sehr gut, wenn man Software oder Plattformen wie Github oder Gitlab als zentrales Gitrepository Managementtool verwendet. Code Reviews erhöhen nicht nur die Qualität des Codes, sondern streut das Wissen unter den Entwicklern.

Ein Versionskontrollsystem zu verwenden ist unbedingt notwendig. Wenn Sie kein VCS verwenden wird Ihr Projekt scheitern. Wichtig ist aber auch dass Sie eine passenden Branching Mering Strategie nutzen.

Feature Based Branching Merging Strategie

Das VCS lässt einem viele Freiheiten. Deswegen definieren die Projekte eine Branching Merging Strategie Im schlechtesten Fall gibt es einen einzigen Branch in dem alle Entwickler direkt einchecken. Das führt normalerweise zu Chaos.

Deswegen sollte man mit einer Feature based branching merging Strategie starten. Diese sieht auf den ersten Blick etwas komplizierter aus: 

Abbildung 13 Feature based branching merging strategy

Der Master ist immer eine stabile aktuelle Version, in die kein Entwickler direkt einchecken darf. Sie sollten jederzeit in der Lage sein den Masterbranch an den Kunden auszuliefern. Jedes Feature wird in einem eigenen Branch entwickelt. Wenn ein Feature fertig entwickelt ist wird es getestet, gereviewed und erst dann darf es in den Master gemerged werden. Das Vorgehen hat zwei Vorteile:

- Die Testphase bevor ein neues Release erstellt wird, kann von den Entwicklern für neue Features für das nächste Release genutzt werden
- Falls sich ein Release verzögert kann man trotzdem ein neues Release ausliefern. Dann liefert man das verzögerte Feature eben im nächsten Release mit aus.

Das ist ein gängiger Weg der sehr gut funktioniert. 

Continuous Integration

Mit dem Continuous Integration Prinzip begegnet man dem Problem, dass Entwickler sehr lange Ihre eigenen Features entwickeln und dann auf einmal viel Code in dem zentralen Repository ändern. Wenn das jeder Entwickler macht führt das oft zu Mergekonflikten. Die Auflösung führt oft zu weiteren Fehlern oder zu weiteren Mergekonflikten. Deswegen werden die Entwickler dazu aufgerufen ihren Code in kleinen Portionen ständig in das zentrale Respository zu integrieren. Es hilft den Entwicklern dabei wenn man neue Features so klein wie möglich schneidet. Um die Code Qualität zu erhöhen wird der Code zusätzlich auf einem unabhängigen Server gebaut. Dazu gibt es sogenannte Building Pipelines, das sind Programme, die man so einrichten kann, dass sie Code bauen, die automatischen Tests laufen lassen und im Fehlerfall  Rückmeldung geben. Beispiele sind Jenkins, Github Actions, Azure Devops usw. 

Früher haben Entwickler oft produktive Releases auf ihrem Arbeitsrechner gebaut wo viele Tools und Libraries vorinstalliert waren. Dieser Release wurde an den Support übergeben der das Ganze auf einen Server installiert hat und das Programm ist abgestürzt, weil der Server bestimmte Voraussetzungen nicht erfüllt hatte. Das kam wirklich oft vor. Die Building Pipeline verhindert so etwas. In Verbindung mit Testautomatisierung ist eine Building Pipeline wirklich mächtig um einen schnellen Projektfortschritt auf einer hohen Qualität über längere Zeit zu garantieren.

 

Die Building Pipelines werden automatisch gestartet, wenn man Code in das zentrale Repository eincheckt. Falls etwas schief läuft, wird der Entwickler benachrichtigt und er muss zusehen, dass er seinen Fehler korrigiert. 

Continuous Deployment

Wenn man jetzt eine Building Pipeline für das Continuous Integration gebaut hat, dann kann man auch noch einen Schritt weiter gehen und das Deployment, also das Bereitstellen eines Releases in die Building Pipeline einbauen. Das nennt man dann Continuous Delivery und im gesamten eine CI/CD – Pipeline.

Der ganze Prozess vom Code einchecken hin bis zum Bereitstellen zum Kunden wird damit automatisiert und spart, wenn es gut funktioniert, richtig viel Zeit. Einer der Gründe warum manche Firmen Feature um Feature generieren die auch noch gut funktionieren. Neue Features werden direkt nach dem Erstellen direkt an den Kunden ausgeliefert. Es gibt tatsächlich Untersuchungen die zeigen, dass umso öfters man deployed, umso stabiler ist die Software. Das funktioniert vor allem im Webumfeld sehr gut. Bei OnPremise – Software ist das schwieriger da man nicht jeden Tag mehrere Updates beim Kunden möchte. Deswegen sammelt man hier etwas länger und stellt neue Release Versionen zur Verfügung. Bei einer Webseite merkt der Kunde gar nicht, dass es ein Update gegeben hat. Deswegen werden viele Anwendungen im Webumfeld programmiert.

Deployment

Unter Deployment versteht man das Ausliefern der Applikation an den Kunden. Hier unterscheidet man grob zwischen zwei Möglichkeiten.

OnPremise vs Online

**OnPremise** sind Programme die direkt auf dem PC installiert werden, wie z.B. Officeprodukte oder auch Spiele. Die Installation geht über ein Installationsprogramm oder über den Store des Betriebssystems. Auf dem Betriebssystem müssen alle Abhängigkeiten installieren werden, die die App benötigt. Umso weniger Abhängigkeiten die App hat umso besser. Ein Programm das in Java programmiert ist, benötigt auf dem Rechner auch Java. Der Kunde muss aber aktiv die Installation anstoßen. Updates müssen auch aktiv angestoßen werden. Hier hat sich die letzten Jahrzehnte nur geändert, dass man nicht mehr CDs ausliefern muss, sondern dass das Installationsprogramm und Updates über das Internet bezogen werden.

**Online** ist in dem Fall etwas spannender da Sie keine Installation beim Kunden benötigen, sondern der Kunde im Browser die Applikation öffnet. Der Browser ist eine Applikation welches basierend aus der Beschreibungssprache HTML, der Styles in CSS und der Programmiersprache Javascript komplexe graphische Userinterfaces (UI) anzeigen kann. Die UI wird dabei nicht einmal lokal installiert, sondern jedes Mal bei Benutzung neu von einem Server ausgeliefert. Der Kunde kann über eine URL im Browser diese Applikation nutzen. Wenn der Nutzer die URL eingibt, verbindet sich der Browser mit einem Server und bekommt die UI ausgeliefert. Der Vorteil ist, Sie müssen keine Updates beim Kunden ausrollen. Wenn Sie neue Funktionen haben können Sie die jederzeit auf den Servern im Rechenzentrum deployen, der Kunde bekommt diese sofort, wenn er die URL öffnet. Außerdem liegen die Daten automatisch auf Servern und können somit von mehreren Benutzern gleichzeitig benutzt werden.

Heutige Applikationen basieren oft auf Online. Das liegt daran, dass es einfacher ist viele Nutzer auf verschiedenen Plattformen wie PC oder Handy zu erreichen. Die Nutzer müssen auf ihrem PC nichts installieren können trotzdem die Anwendung sofort nutzen, wenn ein Browser auf dem PC installiert ist.

Um Online - Software zu deployen benötigt man einen Server, der im Idealfall über das Internet erreichbar ist. Auch hier hat sich in den letzten Jahrzehnten viel getan. Schauen wir uns Evolution des Deployments genauer an.

Server

Server nennt man PCs die leistungsstark in Datacentern stehen, also in hoffentlich abgeschlossenen Räumen mit Serverschränken. Diese Server sind an das Netzwerk angebunden und haben die Aufgabe Programme die im Hintergrund laufen sogenannte Services zur Verfügung zu stellen. Services gibt es eine Menge:

- Fileshare, ein Server kann eine zentrale Ablage von Daten sein
- Mailserver, zentral Mails auf dem Server zu speichern
- Intranet, Informationen für alle bereit stellen
- Webserver, ein Service der Webapplikationen bereitstellt die sich ein Browser abholen kann
- Datenbank
- …

Früher wurden auf den Servern direkt das Betriebssystem installiert und darauf die Services. Ist ein Server kaputt gegangen oder wurde mehr Leistung benötigt musste man die ganzen Services umziehen waren die Services einige Zeit nicht erreichbar.

Virtualisierung

Irgendwann wurde angefangen Hardware zu virtualisieren. D.h. dass man Hardware durch Software simuliert. Man sagt dazu auch Virtuelle Maschinen oder VM. Dieses Konzept findet heute sehr viel Anwendung. Man braucht trotzdem die Server in einem DataCenter nur dass darauf kein Betriebssystem sondern ein sogenannter Hypervisor installiert wird. Dieser Hypervisor ermöglicht es durch Software simulierte Hardware laufen zu lassen. Wie das dann aussieht, sieht man in Abbildung 19.  Der Vorteil ist, wenn ein Server kaputt geht, kann man die VM einfach auf einen anderen Server kopieren und starten. Das reduziert die Ausfallzeiten. Außerdem kann man die Auslastung eines Servers erhöhen, wenn man mehrere virtuelle Instanzen auf einer realen Hardware laufen lässt, da Server normalerweise niemals 100% ausgelastet sind. Außerdem kann man einfach neue Server erstellen per Mausklick, wenn neue benötigt werden und die echte Hardware noch genügend Ressourcen bereitstellt. 

Abbildung 14 Layer mit VM

Container

Das Thema der Virtualisierung wurde noch weiter getrieben in sogenannte Container. Das sind sehr leichtgewichtige virtuelle Maschinen mit einem minimalen Betriebssystem, die man vor allem auf Linux Server gut ausführen kann. Das Thema findet heute so viel Verwendung und hat sehr viel Einfluss auf das aktuelle Deployment von Software.

Abbildung 15 Layer Container

Vor der Verwendung von Containern musste, auf das Betriebssystem alle Abhängigkeiten installiert werden, die die Software benötigt. Also wenn man ein Programm in Java programmiert hat, muss Java installiert sein egal das ein Hardware Server oder eine VM ist. Startet man einen Container auf einem Server ist es unnötig den Server vorher zu präparieren. Bei Erstellung eines Containers werden vom Entwickler einfach alle Abhängigkeiten die nötig sind in den Container integriert. Bei dem Java App Beispiel installiert man in den Container einfach Java. Dadurch ist man flexibel, die Qualität der App wird massiv erhört, die Installationszeiten sehr stark reduziert. Außerdem ist es sehr einfach einen Container auf einem beliebigen Server zu starten. Keine Installation benötigt. Der Container wird geladen und direkt gestartet. Container legt man dafür in ein zentrale Repository ab. Beim Starten wird wenn nötig die aktuelle Version geladen und ausgeführt.

Kubernetes

Um eine weltweit skalierende Software wie die großen Softwarehersteller zu programmieren, gibt es aktuell eine neue Entwicklung, und zwar Kubernetes. Kubernetes verwaltet Server und kümmert sich darum, dass Container je nach Anfragen von Nutzern auf die Server so verteilt werden, dass auch sehr viele Anfragen oder Anfragespitzen abgearbeitet werden können. Damit können Sie prinzipiell weltweit skalierende Software für Millionen von Benutzer zur Verfügung stellen.

Als skalierbare Software bezeichnet man eine Software, die eine stark wechselnde Frequenz an Anfragen und Daten verarbeiten kann. D.h. wenn die Software 100 Nutzer oder 1 Millionen Nutzer gleichzeitig bedienen kann, könnte man das eine skalierbare Software nennen. Dabei gibt es zwei Arten von Skalierung:

In der **Vertikalen Skalierung** erhöht man die Hardwareressourcen. Der PC auf dem das Programm läuft bekommt mehr Speicher, neue CPU oder eine schnellere Netzwerkverbindung. Damit kann das Programm mehr Daten und Anfragen in kürzerer Zeit verarbeiten. Diese Skalierung kommt sehr schnell an seine Grenzen.

In der **Horizontale Skalierung** verteilt man die ankommende Last auf mehr Server. Damit ist theoretisch eine unendliche Skalierung möglich. Diese Art der Software ist schwieriger umzusetzen aber gerade in Webtechnologien sehr wichtig. Neue Technologien zielen sehr stark auf diese Art von Skalierung ab.

Cloud

Wenn man jetzt über Deployment spricht ist die Cloud als Technologie nicht mehr wegzudenken. Eine Cloud ist im Endeffekt ein über die Welt verteiltes großes Rechenzentrum mit vielen physikalischen Servern über welches man Software oder Infrastruktur als Service bestellen und nutzen kann. Man nennt das dementsprechend Software as a Service (SaaS) oder Plattform as a Service (PaaS). SaaS bestellt man eine Software wie eine Datenbank oder SpeechToText Services. Mit PaaS kann man sich virtuelle Server bestellen. Die meisten Clouds haben dazu eine Webseite und ein Kommandozeilentool worüber man sehr einfach die Ressourcen bestellen kann. Der Vorteil einer Cloud ist, dass man sich nicht um ein eigenes Rechenzentrum kümmern muss, dass die Services sehr sicher aufgesetzt werden und upgedatet werden, dass man nach Last bezahlt und dass die Rechenzentren der großen Cloud Provider sehr gut skalieren. Man kann also relativ leicht klein starten mit einem Software Projekt und dann sehr stark skalieren. Ein geeignetes weltweit skalierendes Rechenzentrum aufzubauen, das sehr sicher ist und für die Entwickler die notwendigen Ressourcen bereitstellt ist eine sehr große Aufgabe die sich in fast allen Fällen nicht lohnt. Davon sollte man in den meisten Fällen absehen. Wenn man die Daten nicht in die Cloud speichern möchte, kann man sich die Cloud Technologie bei den meisten großen Anbietern auch in das Unternehmensnetzwerk installieren lassen.

Damit schließen wir das Thema Deployment ab und widmen uns weiteren wichtigen Themen in der Software Entwicklung.

Interfaces

Es gibt zwei Arten mit Software zu interagieren. Entweder die Mensch–Software Kommunikation. Das funktioniert über das User Interface oder auch UI. Der Mensch tippt Daten im Browser ein, bekommt Feedback in einer Konsole, Dashboards zeigen den aktuellen Status an, man fliegt in einem Flugsimulator mittels Joystick über die Welt usw. Der Mensch kann über Tastatur, Maus, Touchdisplay, Mikrofon mit der Software kommunizieren.

Abbildung 16 User nutzt einen PC über eine grafische Oberfläche

Eine andere Art der Kommunikation ist die Software–Software Kommunikation. Zwei Programme tauschen Daten aus. Das Programm das Daten oder einen Dienst bereit stellt wird Server oder auch Service genannt, das Programm welches die Daten abruft ist der Client. Hier stellt der Service eine API bereit, ein Advanced Programming Interface. Der Client kann die API des Service an programmieren und damit geht gewohnt auf eine Webseite. Der Browser ist in dem Fall der Client der die API des entsprechenden Webservers aufruft. Der Webserver antwortet indem er den Inhalt der Webseite zum Client schickt. Der Browser kann dann die Daten als Webseite anzeigen. 

Abbildung 17 Ein Programm ruft ein anderes Programm auf

Mit dem Wort Server wie oben auch erwähnt die Hardware gemeint sein, der zentral Daten für Nutzer bereitstellt und nicht von Menschen direkt zum Arbeiten verwendet wird. Verwechseln sie die beiden nicht, es kommt immer auf den Kontext an ob ein Programm oder ein PC. 

Hier gibt es eine Reihe technischer Möglichkeiten wie Daten über eine API ausgetauscht werden können, hier die zwei wichtigsten:

- Programme können Daten über **Dateien** austauschen. Das eine Programm schreibt Daten hinein, das andere Programm liest die Daten aus. Das ist eine unidirektionale Kommunikation die einfach zu implementieren ist aber mehrere Nachteile mit sich bringt. Dateien können nicht gleichzeitig von mehreren Prozessen geschrieben werden, der Client wird nicht informiert wann Daten da sind und muss ständig nachsehen ob neue Daten vorhanden sind und der Client muss selbst erkennen welche Daten neu sind. Nichtsdestotrotz kann es vorkommen, dass man diesen Weg gehen z.B. muss bei Legacy Systemen.
1. 
- Sockets: Programme können Daten über die Netzwerkkarte versenden bzw. sich für ankommende Daten registrieren. Diese Art der hat so viele Vorteile, dass dies heutzutage der bevorzugte Weg ist. Programme die Daten über das Netzwerk austauschen, können, müssen aber nicht auf derselben Hardware laufen was eine horizontale Skalierung ermöglicht, die Kommunikation kann grundsätzlich weltweit über das Internet funktionieren, die Programme werden informiert, wenn Daten da sind, es ist eine bidirektionale Kommunikation.

Wenn zwei Programme miteinander kommunizieren, egal über welche Art und Weise, ist es wichtig, dass diese auch die gleiche „Sprache“ sprechen. Wenn ein Programm Daten zu einem anderen Programm schickt dann muss dieses die Daten auch verstehen. Das nennt man ein **Protokoll**. 

Es gibt standardisierte Protokolle für bestimmte Aufgaben. Hier ein paar Beispiele:

- HTTP ist ein Protokoll, welches für das Abrufen einer Webseite von einem Webserver entwickelt wurde
- IMAP ist ein Protokoll, um Mails abzurufen
- SMTP ist ein Protokoll, um Mails zu verschicken
- SSH ist ein Protokoll, um remote auf einem Server zugreifen zu können 
- MQTT ist ein Protokoll um Daten von IOT-Geräten zu empfangen

Wenn ein Unternehmen eine neue innovative Idee dafür hat, wie man mit Mails noch effektiver arbeiten kann, sollten Sie das IMAP Protokoll verwenden, um Mails von einem beliebigen Mail-Server abzurufen. Diese Protokolle sind standardisiert und dokumentiert in RFCs die im Internet zu finden sind. Es gibt eine Menge von Protokollen für alle Arten von Software. Das Thema Protokolle führt auch immer wieder zur Verwirrung, weil es immer verschiedene Ebenen von Protokollen gibt. Dazu mehr im Thema Internet.

Es gibt auch Protokolle die nicht öffentlich sind, das sind Protokolle die innerhalb von Komponenten einer Software eines Herstellers zur Verwendung kommen.

Falls Sie daran denken eine neue Software vom Markt zu kaufen, sollten man unbedingt darauf achten, dass diese auch eine einfache API bereitstellen mit dem Sie Daten auf einen einfachen Weg auslesen können. Irgendwann kommt der Tag da möchte man die Daten analysieren oder mit einer anderen Software verknüpfen, dann ist eine API essentiell wichtig.

Datenaustauschformate

Wenn man über APIs redet, muss man auch über Datenaustauschformate reden. Wenn zwei Programme miteinander reden, muss die Information, die ausgetauscht wird von beiden Programmen verstanden werden. Hier redet man über die Semantik der Daten. Die Daten die Programm A an Programm B schickt muss ja von Programm B verstanden werden. Dazu gibt es ein paar Formate die solche Daten standardisieren. Hier die wichstigsten:

JSON

JSON JavaScript Object Notation kommt aus der Sprache JavaScript und hat sich als universelles Datenaustauschformat für APIs durchgesetzt. Der Aufbau ist recht einfach:

{ markiert den Anfang eines Objektes

} markiert das Ende eines Objektes

[ markiert den Anfang einer Aufzählung

] markiert das Ende einer Aufzählung

Wenn ein Service Artikeldaten bereitstellt dann könnte die so aussehen. Ein Wert definiert sich durch die Bedeutung des Wertes, also Name oder Preis und dem dazugehörigen Wert durch : getrennt

{

	ArtikelNummer: ‚1039484747‘,

	Name: ‚Hose‘,

	Preis: 10.99,

	Farbe: ‚gelb‘,

}

Man kann aber auch Subobjekte definieren:

{

	ArtikelNummer: ‚1039484747‘,

	Name: „Hose“,

	Preis: 10.99,

	Attribute: [

		{

			Key: „Größe“,

			Value: 36

		},

		{

			Key: „Farbe“,

			Value: „Blau“

		}

	]

}

XML

In XML nutzt man Tags um Daten zu beschreiben. Ein Tag beschreibt die Bedeutung des Wertes und innerhalb des Tags steht der Wert. Man kann verschachtelte Tags schreiben.

<Artikel>

<ArtikelNummer>‚1039484747‘</ArtikelNummer>

	<Name: „Hose“,

	<Preis: 10.99,

	<Attributes>

		<Attribute key=”größe”>36</Attribute>

		<Attribute key=„Farbe“>Blau</Attribute>

	</Attributes>

</Artikel>

Gibt noch mehr modernere Datenaustauschformate wie protobuf oder apache avro. Es lohnt sich diese in einem Projekt, wo viele Services intensiv miteinander kommunizieren müssen diese Formate genauer anzusehen.

Internet

Das Internet wurde in den 70er Jahren erfunden und hat heute einen sehr hohen Stellenwert in der Welt. Wenn Sie sich mit Digitalisierung beschäftigen müssen Sie sich mit dem Internet beschäftigen und dessen Möglichkeiten. Deswegen gehen wir hier auf auch etwas detaillierter auf diese Technik dahinter ein.

Die meiste Hardware die wir kaufen verfügt eine Möglichkeit sich mit einem Netzwerk zu verbinden, sei es über Kabel, Wlan oder Mobilfunk. Die Netzwerkverbindung ist in mehrere Ebenen aufgebaut das sogenannte OSI ISO 7 Schichtenmodell Auf jeder Schicht gibt es verschiedene Protokolle mit verschiedenen Anwendungsfällen die es benötigt damit das Internet weltweit Dienste bereitstellen kann.

Die Schicht 1 definiert wie man mit Kabel, Wlan oder Mobilfunk interagieren muss um eine Verbindung zu bekommen. Die Schichten 2 bis 4 sind nur dazu da um überhaupt die Kommunikation über das Internet zu ermöglichen. Hier sind lokale und globale Adressen enthalten und je nach Protokoll Informationen über die aktuelle Kommunikation, wie die Reihenfolge der Pakete oder auch dass Pakete, die verloren gegangen sind noch einmal angefordert werden. Jede Schicht hat ihre eigenen Protokolle für verschiedene Verwendungszwecke und beinhaltet als Datenfeld die Protokolle der nächsten Schicht.

1. Physik

Wie schon erwähnt handelt es sich hier um das Transportmedium welches die Daten transportiert, z.B. Kabel oder Funk.

2. Ebene Sicherung

Jede Netzwerkkarte hat eine weltweit eindeutige Adresse die vom Hersteller vorgegeben ist, die sogenannte Mac-Adresse. Diese Adresse z.B. 00:80:41:ae:fd:7e wird zur Identifizierung der Netzwerkkarte verwendet, ist fix in der Netzwerkkarte verankert und ist global eindeutig. Die Netzwerkkarte nimmt auch nur Datenpakete an die mit dieser Adresse versehen sind. Hier wird meistens das Ethernet Protokoll verwendet. 

3. Ebene Network

Die Vermittlungsschicht führt eine neue Adresse ein, die IP-Adresse z.B. 192.168.0.101. Jeder Internetteilnehmer braucht eine Internetadresse. Auch wenn alle PCs über das Internet prinzipiell miteinander verbunden sind, wird das Konstrukt in Netzwerksegmente aufgeteilt. Will ich z.B. in meinem Haus mein Smartphone mit meinem Fernseher verbinden läuft diese Kommunikation nur über das Netzwerksegment in meinem Haus ab. Zur Verbindung aller Netzwerksegmente weltweit gibt es ein Gerät man Router nennt. Das hat jeder daheim der einen Internetanschluss hat. Über die IP – Adresse (und der zugehörigen Subnetzmaske) erkennt der PC ob der Zielrechner im gleichen Netzwerksegment ist.

Wenn man jetzt die Anfrage an einen externen Netzwerkteilnehmer sendet, werden diese Daten an den Router geschickt. Der Router kümmert sich für die Weiterleitung der Anfrage. Alle Kommunikation in ein anderes Netzwerksegment. Alle Kommunikation innerhalb eines Netzwerksegments läuft direkt zwsichen den Netzwerkteilnehmer ab. 

Nachdem die IP – Adressen nicht gut zu merken sind gibt es einen Dienst mit dem Namen DNS (Domain Name Server). Dieser löst die Webadressen die wir kennen und im Browser eingeben auf und wandelt diese in IP-Adressen um.

Wenn man eine externe Webseite mit dem Browser aufruft passiert folgendes:

1.1. Der PC fragt an einem DNS-Server nach und bekommt eine IP-Adresse zurück 
1.1. Der PC schaut, ob die IP-Adresse in seinem Netzwerksegment liegt
1.1. Wenn nicht schickt er die Anfrage an den Router
1.1. Der Router versendet die Anfrage an weitere ihm bekannte Router. Die Router sind zusammengeschaltet und über die IP-Adresse findet das Paket durch die Router den Weg zu dem Ziel-Server.
1.1. Der Ziel-Server schickt die Antwort und es geht den gleichen Weg zurück zur IP-Adresse des Routers. Der Router leitet die Antwort an den Rechner weiter, der die Anfrage ursprünglich gestartet hat. Der Router ist der einzige, im Heimnetzwerk der eine offizielle im Internet verfügbare IP-Adresse hat. Innerhalb des Routers werden Standard Subnetz IP-Adressen verwendet wie 192.168.0.200 oder 10.0.0.25

Abbildung 18 Schematische Darstellung der Kommunikation

4. Ebene Transport

Die Transportschicht ist nochmal eine Schicht die für einen geregelten Ablauf der Pakete sorgt. Die Größe eines Paketes ist beschränkt. Man versendet in einem Paket maximal 1460 Bytes an Daten. Wenn man mehr Daten schicken möchte werden diese automatisch gesplittet. Diese Pakete können verloren gehen oder sich gegenseitig überholen. Die Transportschicht mit dem TCP Protokoll weist jedem Paket eine aufsteigende Zahl zu. Dann kann der Empfänger die Daten in den Paketen in die richtige Reihenfolge wieder zusammenfügen oder verloren gegangene Pakete nachfordern. Es gibt auf dieser Ebene auch andere Protokolle die für Gaming oder Streaming besser sind, weil es hier irrelevant ist, dass man die Pakete in der richtigen Reihenfolge bekommt bzw. ein Paket verloren geht. 

Zusätzlich dazu führt TCP noch eine Nummer ein, die man **Port** nennt. Der Port ist dazu da, dass man über eine IP-Adresse mehrere Services anbieten kann. Wenn man z.B. auf einem Server eine Webseite und einen Mail-Server bereitstellen möchte, man hat aber nur die eine IP – Adresse, woher soll der PC wissen für welchen der Dienste das ankommende Datenpaket gedacht ist. Der Port ist eine Zahl von 1-65536 und gibt an welchen Dienst man ansprechen will.

Erst innerhalb des TCP Protokolls kommen Anwendungsprotokolle wie HTTP oder IMAP zum Tragen und erst dann kommen die eigentlichen Nutzdaten.

Abbildung 19 Über Ports kann man mehrere Netzwerkdienste auf einem Server laufen lassen

Also nochmal zusammengefasst an einem Beispiel:

Wir wollen eine Webseite besuchen, dazu gibt es zwei Teilnehmer, der Browser und der Webserver des Anbieters. Der Browser und der Webserver beide kommunizieren über das HTTPS Protokoll. Der Server verfügt über eine im Internet eindeutige IP Adresse und Webserver werden normalerweise mit dem Port 443 gestartet. Damit die Webseite einen sprechenden Namen hat beantragt man eine Domain wie z.B. endofweb.de. Diese Domain wird einer IP-Adresse zugewiesen und in den DNS -Servern gespeichert.

Ein Nutzer der die Webseite besuchen möchte gibt im Browser den Domain Namen ein. Bei Eingabe des Domainnamens wird die IP-Adresse aufgelöst über einen DNS Server und eine Anfrage über den Router an die IP-Adresse mit dem Port 443 ausgelöst und zwar so wie das HTTPS Protokoll es vorschreibt. Die Antwort des Servers auf die Anfrage ist auch HTTPS konform und schickt die Webseite als Daten zurück. Die Daten werden in kleine Netzwerkpakte aufgeteilt die über das TCP Protokoll beim Empfänger wieder zusammengesetzt werden. Die Netzwerkanfrage kann über verschieden physikalische Medien wie Netzwerkkabel oder Wifi passieren. 

Abbildung 20 Anfrage Webseite

Datenbanken

Die meisten Anwendungen benötigen eine Persistierung der Daten. D.h. dass die Daten auf der Festplatte gespeichert werden und auch beim Schließen und Öffnen der Anwendung wieder vorhanden sind. Dazu kann man selber die Daten in Dateien speichern, das wird aber relativ schnell aufwendig und ineffizient. Deswegen nutzt man in der Softwareentwicklung Datenbanken. Das sind Services die Daten sehr effizient auf die Festplatte schreibt und auch wiederfindet. Auf die Daten kann man über die API zugreifen und CRUD Operationen ausführen kann:

- **C**reate, das Schreiben von neuen Daten
- **R**ead, das Lesen von einem oder mehreren Werten. Dazu gibt es oft vielfältige Filtermöglichkeiten
- **U**pdate, das Verändern von schon vorhandenen Daten
- **D**elete, das Löschen von Daten

Die meisten Datenbanken erlauben es mehrere Replikationen anzulegen. Das bedeutet, dass man eine Datenbank über mehrere Server verteilt. Die Replikationen synchronisieren sich ständig so dass man eine vertikale Skalierung hinbekommt. Wenn Sie z.B. eine Messanger App mit Millionen Nutzern gleichzeitig schreiben möchten kommen Sie um sowas nicht herum.

Die Datenbanken nehmen gerade in der Applikationsentwicklung eine sehr zentrale Rolle ein. Deswegen ist wichtig hier die richtige Datenbank auszuwählen.

Es gibt verschiedene Arten von Datenbanken und eine Einteilung ist SQL und NoSQL Datenbanken. 

- SQL ist die Abfragesprache die sich hartnäckig seit Jahrzehnten hält
- NoSQL heißt „Not only SQL“, diese Datenbank haben oft eigene Abfragesprachen unterstützen teilweise aber auch SQL.
1. 

Betrachten wir die vier Haupttypen von Datenbanken, wobei die am häufigste verwendete die relationale Datenbank ist. Somit werden wir diese genauer betrachten. Die anderen werden wir nur kurz anschneiden.

Relationale Datenbanken

Relationale Datenbanken legen die Daten in Tabellen ab, die man miteinander verknüpfen kann. Die Verknüpfung nennt man auch Relation. Die Tabellen müssen modelliert werden. Die Spalten der Tabelle bekommen einen bestimmten Namen und einen Typen zugewiesen. Man kann keine Daten in die Datenbank schreiben die nicht mit den Tabellendefinitionen exakt übereinstimmen. Wenn man über Anforderungen das Datenbankschema ändern möchte muss man je nach Art der Änderung die Daten migrieren, also SQL Skripte schreiben die eine neue Tabelle anlegen oder eine Spalte ändern oder auch Daten umkopieren. Das kann eine Weile dauern und dazu führen, dass Ihre Applikation lange benötigt um zu starten.

Datenmodellierung:

Auf die Modellierung des Datenmodells in relationalen Datenbanken legen Entwickler immer besonders großen Wert. Hier wird mit einem Entity – Relationship – Model die Tabelle und ihre Relationen festgelegt. Hier gibt es Regeln die es zu beachten gilt, sogenannte Normalformen. Müssen Sie sich nicht merken, Hauptsache sie haben es mal gehört. Z.B. möchte man das obige Beispiel Kunde, Artikel und Bestellung als relationales Datenbankmodell designen würden drei Tabellen herauskommen:

Abbildung 21 ER - Modell

 

Man könnte prinzipiell wie in Excel auch alles in eine Tabelle schreiben würde aber zu einer massiven Redundanz der Daten führen. Also wenn man Bestellung und Kunde in eine Tabelle packt hätte man den Kundenamen pro Bestellung einmal kopiert.

Die Relationen sind als Verbindungen zwischen den Tabellen dargestellt. In Wahrheit sind das Nummern oder IDs. D.h. ein Kunde bekommt eine eindeutige Nummer auch Primary Key genannt. Eine Bestellung bekommt auch eine eindeutige Nummer aber zusätzlich die Nummer des Kunden in einer Spalte eingetragen so dass man die Zuordnung hat. Die Relationen nennt man auch Foreign Keys oder Referenzen.

In dem Beispiel steht jetzt z.B. der Kunde Florian mit der ID 1 hat eine Bestellung (Spalte KDNR) mit der ID 3 ausgelöst und da 5 Stück eines Artikels mit der ID 1 bestellt, eine Hose. 

Dieses Datenbankmodell das wir hier gewählt haben hat mehrere Probleme: 

- Sie können jetzt in dem Zustand bei der Bestellung nicht einfach ein Datum speichern ohne nicht vorher die Tabelle zu ändern. Das bedeutet eine Migration der Tabelle weil relationale Datenbanken ein striktes Schema haben von dem Sie nicht abweichen können
- Wenn sich der Name eines Kunden ändert würde sich das auf alle Bestellungen auswirken die jemals von dem Kunden getätigt würde. Das darf in dem Fall natürlich nicht passieren
- Artikel haben so viele unterschiedliche Attribute die können Sie nicht alle als Spalte in das Schema mit aufnehmen können. Deswegen haben E-Commerce Systeme die mit relationalen Datenbanken arbeiten alleine für die Artikel mit den Attributen 6 Tabellen.

Nachdem man Daten immer Tabellen aufteilt, kann es vorkommen, dass man für einen Datensatz mehrere Schreibbefehle auslösen muss. Wenn man in 3 Tabellen Daten schreibt und 1 Schreibbefehl fehlschlägt dann hat man inkonsistente Daten. Deswegen gibt es sogenannte Transaktionen in dem man mehrere Befehle zusammenfasst und erst wenn alle Befehle wirklich funktioniert haben kann man mit einem Commit das endgültige Speichern veranlassen.

Wenn Sie eine Applikation haben die viele Daten in kurzer Zeit wegspeichern muss, kann es sein, dass Sie von der Normalform abweichen müssen und Tabellen denormalisieren müssen. (Diesen Begriff gibt es tatsächlich). Sie schreiben die Daten einfach in eine große Tabelle und akzeptieren die Redundanzen die darin vorkommen können. Wenn Sie die Daten aufteilen auf mehrere Tabellen wollen, müssen Sie eine Transaktion starten und mehrere Schreibbefehle auslösen. Das kann bei vielen Daten zu Verzögerungen führen.

Die Befehle die man auslöst werden in der Sprache SQL geschrieben, hier ein paar Beispiele:

„SELECT * FROM Bestellung“

Man bekommt eine komplette Liste aller Einträge in der Tabelle Bestellungen

„SELECT * FROM Kunde INNER JOIN Bestellung ON Kunde.ID=Bestellung.KDNR“

Man verknüpft die Tabelle Kunde mit der Tabelle Bestellung und bekommt eine komplette Liste aller Bestellungen mit zugehörigen Kundennamen

„SELECT * FROM Kunde INNER JOIN Bestellung ON Kunde.ID=Bestellung.KDNR WHERE Kunde.ID = 1“

Man filtert auf alle Bestellungen auf den Kunden mit der ID 1. Man kann auch auf Namen filtern

„INSERT INTO Artikel (Name) VALUES (‚Buch‘)“

Man fügt einen neuen Wert in die Tabelle Artikel mit dem Namen Buch. Die ID wird automatisch generiert wenn man das möchte

„UPDATE Artikel SET Name=“DochKeinBuch“ WHERE ID=1“

Man ändert den Namen des Artikels mit der ID 1 wird auf DochKeinBuch geändert. Wenn man das WHERE weglässt updatet man alle Artikel auf einmal.

„DELETE Artikel WHERE ID = 1“

Löscht den Artikel ID 1 ist. Hier müssen Sie aufpassen, dass wenn Sie einen Artikel löschen aber in der Bestellung hier noch eine Relation (auch Referenz genannt) haben, dann haben Sie eine Inkonsistente Datenbank.

Ein Entwickler muss in seinem Programm Strings generieren die SQL-Befehle darstellen und das ganze über die Datenbank API an die Datenbank schicken. Das ist umständlich deswegen gibt es sogenannte OR-Mapper. Das sind Libraries die einem Entwickler Tabellen aus der Datenbank mit Objekten in seiner Programmiersprache mappen. Dann muss sich der Entwickler nicht mehr selber darum kümmern, wie man SQL-Befehle generiert.

Zusätzlich dazu gibt es in den meisten relationalen Datenbanksystemen die Möglichkeit Stored Procedures zu schreiben. Das sind Funktionen die mit SQL programmiert werden und innerhalb der Datenbank laufen. 

Mit Stored Procedures wurden Anfang 2000er Jahre viele Applikationen sehr schnell umgesetzt. Allerdings sollten Sie das unbedingt vermeiden wo Sie können. Das wird nach einigen Jahren in Probleme führen. SQL bietet einfach nicht genug Features um eine geeignete Software Architektur aufzusetzen für komplexe Applikationen. Außerdem verlieren Sie die Skalierbarkeit Ihrer Software und automatische Tests sind schwieriger umzusetzen.

Document based

Dokumentbasierende Datenbanken arbeiten nicht mit Relationen und auch nicht mit Tabellen. Es gibt Collections. Man kann z.B. eine Collection mit dem Namen Artikel anlegen. In diese Collection kann man jetzt JSON abspeichern. Dieses JSON ist aber Schemalos, d.h. man muss die Struktur vorher nicht modellieren und festlegen. Hier werden einfach JSON Objekte gespeichert. Die Datenbank geht einfach davon aus, dass der schreibende Prozess weiß, was er da reinschreibt. 

Sehen Sie wo der Charme liegt in schemaloses Speichern von Daten im JSON – Format? Bei Artikeln können Sie einfach die Attribute so festlegen ohne sich um ein Schema kümmern zu müssen. Sie können alle Artikel egal welche Attribute diese haben in eine Collection packen und mit einem Lesebefehl alle Artikel auslesen. Das würde über eine relationale Datenbank nicht so einfach funktionieren. Document based Datenbanken sind geeignet, um schnellen Fortschritt am Anfang eines Projektes zu erzeugen. Sie müssen sich nicht um ein Datenbankmodell kümmern und bei Änderungen müssen das Schema nicht anpassen. Gerade am Anfang eines Projekts ändert sich das Datenmodell sehr oft. Aber es können auch mehr Fehler auftreten, weil die Datenbank sich nicht um das Schema kümmert und Sie so nicht immer sicher sein können, dass die Daten in dem Format vorliegen, welches sie erwarten.

Diese Datenbanken legen jetzt auch nicht so sehr viel Wert auf Konsistenz der Daten so dass Sie sehr gut vertikal skalieren. Es kann Ihnen passieren, dass ein Wert in Replikation 1 gespeichert wird und dieser erste ein paar Sekunden danach in allen Replikationen vorhanden ist.

Graph Datenbanken

Graphen sind in der Informatik Gebilde die aus zwei Dingen besteht. Knoten und Verknüpfungen. Die Verknüpfungen können eine Richtung haben und gewichtet sein. Manchmal macht es Sinn so etwas einzusetzen. Z.B. wenn Sie ein Navigationssystem bauen möchten. Straßen sind Verknüpfungen, die eine Richtung haben und mit einem Wert gewichtet sind. Kreuzungen sind Knoten. So können Sie mit verschiedenen Algorithmen einen Weg von A nach B finden.

Das wird vor allem da eingesetzt, wo es komplexe Abhängigkeiten zwischen vielen Objekten gibt. Graph Datenbanken sind sehr gut geeignet solche Daten zu speichern. 

Big table

Reden wir von Big Data, also das Erfassen von vielen Daten reden wir über Big Table Databases. Diese Datenbank sind dafür ausgelegt sehr viele Daten zu schreiben und zu verarbeiten. Hier gibt es auch Tabellen aber diese werden anders modelliert. Sie können bei diesen Datenbanken auch nicht so einfach ein Update eines bestimmten Datums machen, da ein Update immer das Durchsuchen der Daten und das Schreiben beinhaltet. Das würde das Wegschreiben von vielen Daten sehr langsam machen. 

Solution Architektur

Wenn Sie eine Anwendung programmieren möchten, benötigen Sie einen Plan. Die Idee, die Sie umsetzen wird in grobe Anforderungen beschrieben. Dies passiert vor allem auf der fachlichen Seite. Aber auch auf technischer Seite benötigt man einen Plan. Das nennt man Softwarearchitektur. In der Softwarearchitektur müssen alle Themen, die wir kennen gelernt haben, bewertet und zu einem Konzept zusammengefasst werden. Welche Programmiersprache verwendet man, wie teilen Sie den Code in Libraries auf, wie sehen die API aus, welche externen Systeme müssen Sie anbinden, wie rollen Sie die Software aus. Diese Fragen müssen so beantwortet werden, so dass es den Anforderungen entspricht und dass die Software leisten kann was Sie von ihr erwarten.

Die Architektur zu designen ist normalerweise eine Aufgabe für einen Senior Softwareentwickler oder dedizierten Softwarearchitekten. Diese müssen ein breites Wissen über verschiedene Technologien verfügen, um einschätzen zu können, welche davon die Richtige ist. Im Idealfall kann wird auch die Organisation des Unternehmens und die Fähigkeiten des Teams berücksichtigt. Es gibt normalerweise in jedem Team einen Verantwortlichen für die Softwarearchitektur der sich dauerhaft um die Planung kümmert, diese weiterentwickelt, anpasst und dokumentiert. Dieser kann und sollte auch in dem Projekt mitentwickeln. Bei größeren Projekten kann es dedizierte Softwarearchitekten geben die teamübergreifend die Architektur planen. Außerdem ist er erster Ansprechpartner für den Product Owner und dem Kunden, wenn es um technische Themen geht, sollte deswegen über gute kommunikative Fähigkeiten verfügen. Um Architektur zu dokumentieren, gibt es die Beschreibungssprache UML.

UML 

UML (Unified Modeling Language) ist eine Beschreibungssprache für verschiedene Anwendungsszenarien und hat sich bei der Dokumentation von Solution Architekturen durchgesetzt. Hier sind folgende Diagramme von Interesse:

- **Use Case Diagramm**: Hier kann man Use Cases beschreiben. Ein Use Case ist eine Anforderung die den Nutzer mit einbezieht. 

(Bild)

1.  
- **Klassendiagramm**: Im Klassendiagramm kann man die Typen und Klassen, die man in der Anwendung entwickelt modellieren. Damit ist auch die Modellierung 

(Bild)

1. 
- **Komponentendiagramm**: Das Komponentendiagramm ist ein Blick aus einer höheren Ebene. Komponenten können Bibliotheken sein, die man zueinander in Beziehung setzt

(Bild)

- **Sequenzdiagramm**: Das Sequenzdiagramm modelliert die Kommunikation zwischen mehrere Parteien. Das können User sein, oder Services die über eine API miteinander interagieren.

Pattern

Es gibt in der Architektur Best Practices, sogenannte Entwurfsmuster oder Patterns. [https://de.wikipedia.org/wiki/Entwurfsmuster](https://de.wikipedia.org/wiki/Entwurfsmuster). Die Idee kommt zwar von der Bauarchitektur hat sich dort aber nicht so durchgesetzt. In der Informatik wurde die Idee aufgegriffen und findet breite Anwendung

Ein Pattern ist eine Lösung für wiederkehrende Probleme für die es aber keine standardisierte Lösung geben kann. Die meisten Patterns basieren auf Erfahrungen der letzten Jahrzehnte aber auch auf Architekturen der großen Softwarecompanies. In der Software Entwicklung ist es üblich Wissen zu teilen und so verbreiten sich auch Best Practices in der Software Architekturen. Wir werden hier einmal die drei Schichten Architektur basierend auf einem monolithischen Ansatz und auf Microservices durchgehen. Vorher noch ein paar Worte zur Architektur allgemein.

Enterprise Architektur

In großen Firmen gibt es oft eine Abteilung die sich um übergreifende Zusammenhänge innerhalb des Software Portfolios einer Firma kümmert. Die Enterprise Architekten schauen sich die Software Landschaft der Firma an und bauen daraufhin eine Enterprise Architektur auf wie man diese vereinheitlichen und standardisieren kann, wo noch Lücken im Portfolio sind und wie die Kommunikationsströme sind. Eine sehr hohe Sicht auf das ganze Unternehmen mit wenig Einfluss auf die Software Architektur bzw. die Solution Architektur.

Architekturpatterns

Welche Architektur man verwendet ist sehr abhängig von den Anforderungen und den Rahmenbedingungen. Früher waren Programme oft monolithisch, man hat den Code alles zusammen in ein großes Programm gepackt. Das ist auch heute noch ok wenn man normalerweise einen Nutzer der lokal am Rechner die Features nutzen möchte und dabei nicht parallel mit mehreren anderen Nutzern auf der gleichen Datenbasis arbeitet. Das sind z.B. Officeprodukte wie Textverarbeitung oder Malprogramme bzw. Soundprogramme, also Programme die man lokal installiert und von einem Nutzer verwendet werden. Da gibt es auch viele Pattern wie das MVC Pattern wo man die Daten von der Visualsierung und der Eingabe in drei Bereiche teilt. Der Trend geht trotzdem in eine zentrale Architektur in der man in Kooperation mit anderen gleichzeitig auf gemeinsamer Datenbasis arbeitet.

Drei Schichten Architektur

Die Drei Schichten Architektur besteht aus einer Datenbank, einem Backend und einem Frontend. Diese Architektur ist vor allem für einfach skalierbare Webanwendungen eine gute Idee. Die Datenbank speichert die Daten effektiv ab, das Backend enthält die Businesslogik und das Frontend bietet dem Nutzer die Möglichkeit der Interaktion. Durch die Trennung des Frontends, also der UI von dem Backend ist das für Multiuserszenarien sehr gut geeignet. Und es ist relativ einfach auf verschiedenen Geräten die Anwendung zu nutzen.

Monolithisch

Packen Sie jeglichen Code den Sie haben in eine Anwendung nennt man das Monolithisch. Im Prinzip bedeutet das, dass wenn der Nutzer die Applikation startet fast jeder Code in einem oder wenigen Prozessen läuft. Bei großen Anwendungen kann diese Art der Entwicklung recht unübersichtlich werden, aber recht einfach in der Umsetzung wenn man sich an eine klare Software Architektur hält.

Microservice  

Wenn Sie eine weltweit skalierende Anwendung schreiben wollen, die robust funktioniert, wo mehrere Teams gleichzeitig sehr schnell neue Features umsetzen können sollten sie einen Blick auf das Microservices Pattern richten. In diesem Pattern wird eine Anwendung in kleine voneinander unabhängige Services verpackt. Jeder Service ist wenn nötig in einer 3 Schichten Architektur aufgebaut. Die Frontends der einzelnen Services werden am Ende in eine Anwendung zusammengefügt, so dass der Kunde nicht merkt, dass er eigentlich mit sehr vielen verschiedenen Services läuft. Mit dieser Architektur können sie sehr stark skalieren. Sie können in komplexen Applikationen sehr schnell über viele Teams neue Features umsetzen. Das klingt so gut, dass viele Projekte mit dieser Architektur arbeiten. Aber diese Architektur bedingt ein Zusammenspiel von sehr guten Teams, von guten agilen Organisationsstrukturen und dem DevOps Gedanken. Sie benötigen Teams die wirklich selber in agilen Prozessen arbeiten können und moderne Software Entwicklung beherrschen und die Verantwortung für ihre Services übernehmen wollen, genauso wie eine Organisation die mit der Microservice Architektur abgestimmt ist und die es zulässt dass Teams die Verantwortung übernehmen dürfen. Wenn Sie in hierarchischen Strukturen arbeiten ist das meist ein Indikator dafür, die Finger von Microservices zu lassen. Bauen Sie lieber ein monolithisches System mit einer sauberen Architektur und trennen Sie diesen Monolithen in einem späteren Schritt auf. Wenn Ihnen jemand die Vorzüge der Microservice Architektur anpreist sollten bei Ihnen die Alarmglocken läuten.

Conways Law

Das Gesetz von Conway entstand von Melvin Conway und sollte jeder Manager kennen. Im Prinzip ist es das wichtigste Kapitel, dass Sie hier als Manager im Buch finden werden. Und wenn nichts von der Technik hängen bleibt, dann bitte merken Sie sich Conways Law. Melvin Conway hat in Studien vereinfacht festgestellt, dass die Architektur einer Software immer der Organisation folgt:

Any organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization's communication structure

— Melvin E. Conway

_D.h. der Solution Architekt ist nicht frei in seiner Architektur, er muss Sie prinzipiell an die Organisation anpassen. Wenn Organisation und Architektur zu stark voneinander abweichen wird Ihr Projekt über kurz oder lang in Schwierigkeiten geraten. D.h. Sie sollten Ihre Organisation in enger Abstimmung mit dem Architekten gestalten auch wenn Ihnen das seltsam vorkommen mag._

Daten

Wer Digitalisierung vorantreiben wird um das Thema Daten nicht herumkommen. Daten sagen viel über Ihr Unternehmen aus. Mit Daten kommt die Transparenz und die Möglichkeit zur Optimierung. Wollen Sie wirklich wissen was ein einem Unternehmen läuft, ungeschönt und komplett neutral bewertet. Daten oder KPIs sind objektiv, basierend auf Fakten und nicht auf Bauchgefühl. Menschen tendieren verständlicherweise immer dazu sich selbst positiv darzustellen und das dann dazu führen, dass man nie genau was wirklich im Unternehmen läuft. Nach oben werden Projektstatus als grün dargestellt oder jeder hofft, dass ein anderes Projekt dem Topmanagement beichtet wen es nicht gut läuft. Wenn alle Prozesse digitalisiert sind, wenn alle Daten zentral ausgewertet werden und die richtigen KPIs berechnet werden, ermöglicht dass genau zu sehen wo es gut läuft und wo nicht.

Also beginnen Sie Daten zu sammeln und zu zentralisieren und zwar noch heute.  Spüren Sie Ihre Datensilos auf und versuchen Sie einen großen Datentopf (DataLake) zu generieren, der über alle Bereiche Ihres Unternehmens geht und in dem Sie die Daten zentralisieren. Dafür können Sie Data Engineers einstellen. Dieser neue Beruf kümmert sich darum Daten zu sammeln und sie in dem Datenmodell abzuspeichern wie ein Data Architect es vorgibt. Damit werden die Daten verständlich. Der nächste Schritt ist es aus den Daten Erkenntnisse ziehen und damit Kosten zu sparen oder neue Businessmodelle aufzubauen. Das macht der Data Analyst. Auch dieser Beruf ist eher neu. Mit statistischen Methoden wird versucht Beziehungen aus den Daten herstellen zu können und sinnvolle Reports zu generieren. Ob Sie wirklich alle drei Berufe benötigen, hängt von der Größe des Unternehmens und der Stand der Digitalisierung hab. Ist es einfach die Daten zusammen in einen Data Lake zu schreiben, weil Sie schon alles in Applikationen mit einer guten Schnittstelle ablegen, dann werden Sie schnell erste Erkenntnisse bekommen. Wenn Sie noch viele Prozesse mit Papier und Excel führen, kann das lange dauern und sehr aufwendig werden. Hier ist die Devise in kleinen Schirtten anzufangen. Suchen Sie sich einen Bereich der für Sie intransparent ist oder wo sie das Gefühl haben dass etwas nicht so läuft wie es möchten und versuchen Sie da die ersten Daten zu bekommen. 

Wenn wir die Sicht von dem Unternehmen auf extern richten, vielleicht haben Sie die Möglichkeit Daten über Ihre Produkte oder Dienstleistung zu bekommen und diese für neue Businessmodelle zu nutzen. Die Großen IT Firmen wie Google oder Facebook bekommen Ihre Daten von den Kunden sozusagen bereitwillig hingeworfen. Die Stauerkennung in Google Maps ist da nur ein Beispiel. Vielleicht können Sie Sensoren in ihre Produkte anbringen die die Daten in Ihre Cloud schreiben oder Sie können das Verhalten Ihrer Kunden über eine App analysieren. Damit können Sie neue Businessmodelle finden die sie jetzt noch nicht kennen.

Umso mehr Daten Sie haben umso mehr Insights also Einsichten können Sie generieren. Deswegen gibt es aktuell eine Datensammelwut. Das fällt auch unter den Begriff Big Data. Mit den Daten können Sie aber nicht nur Insights in Kundenverhalten oder Ihr Unternehmen bekommen. Sie können damit auch Software generieren die Ihre Digitalisierung weit voranbringen kann, die künstliche Intelligenz.

Künstliche Intelligenz

In der Digitalisierung erhofft man sich viel von dem Thema Künstliche Intelligenz. Es gibt immer wieder Meldung welche Fortschritte die Künstliche Intelligenz macht. Was steckt da dahinter.

Das Thema KI ist riesig und hat sehr viel mit Mathematik zu tun. Es gibt verschiedene Arten von künstlicher Intelligenz, aber wir reden hier vor allem über neuronale Netze, also der Versuch so eine Art Gehirn nachzubilden. Man weiß wie Neuronen im Gehirn funktionieren. Es ist eine Zelle die durch Stromimpulse angeregt selber Stromimpulse aussenden kann, wenn der Eingangsstrom einen bestimmten Wert übersteigt. Das kann man in einer mathematischen Formel ausdrücken. Zwischen den Neuronen gibt es Verbindungen, die Synapsen, die die Stromimpulse weiterleiten. Das Gehirn kann neue Dinge lernen aber auch verlernen. Neue Neuronen und Synapsen werden gebildet aber auch wieder abgebaut, wenn sie nicht mehr verwendet werden. Übt man ein Musikinstrument immer und immer wieder werden neue Verbindungen im Gehirn geschaffen und wenn man es dann aufgibt, bleibt ein Rest zurück. D.h. man verlernt es nicht, aber man wird schlechter.

Dieses Prinzip versucht man nachzuahmen, indem man Neuronen und Synapsen simuliert. Das geht recht einfach mit mathematischen Matrizen. Aber die Mathematik lassen wir außen vor.  Diese Verbindungen werden mit einer Zahl belegt sind also gewichtet. Die Gewichtung wird je nach Training erhöht oder erniedrigt und stellt dar wie gut zwei Neuronen miteinander verbunden sind. Damit schafft man es tatsächlich so eine Art Gehirn nachzubilden mit dem man erstaunliche Fähigkeiten in eine Software bringen kann.

Abbildung 22 Schematische Darstellung eines Neuronalen Netz

Die Gewichtung zwischen den Neuronen und das ist wichtig, wird nicht vom Entwickler festlegt, sondern trainiert. Das ist eine andere Art von Software Entwicklung für die sich ein eigenes Berufsbild entwickelt hat, die Datenanalysten. Ist das Training abgeschlossen wird das neuronale Netz in einer Software integriert was mit einer API Daten entgegennimmt, dem Neuronalen Netz übergibt und das Endergebnis zurückliefert. 

Für das Training eines Neuronalen Netzes benötigt man Daten um so mehr Daten man hat umso besser. Ein relativ einfaches Neuronales Netz sind die sogenannten Classifier. Hier wird versucht einen Eingangswert in Kategorien einzuteilen. Oft wird zur Erklärung das Beispiel zur Erkennung von Hunden und Katzen auf Bildern zurückgegriffen. Sie haben ein Bild von einem Tier und möchten wissen ob das ein Hund oder eine Katze ist. Um ein Modell zu trainieren, benötigt man viele Bilder von Hunden und Katzen. Diese Bilder müssen gelabelt sein, das bedeutet, dass die Trainingsbilder auch ihre Bedeutung mitbringen. Also Bild A -> Katze, Bild B -> Hund, Bild C -> Katze usw.. Das Ziel ist es das Modell so zu trainieren, dass es Hunde und Katzen auch dann noch erkennt, wenn es keines der Bilder ist, mit denen wir das Modell trainiert haben.

Die Trainingsphase führt dazu, dass sich die Werte in dem Neuronalen Netz so ändern, dass es am Ende die Merkmale, woran man eine Katze und einen Hund erkennt in sich enthält. Im Endeffekt ist es das gleiche was wir Menschen machen. Ein kleines Kind sieht etwas und die Eltern erklären: Das ist ein Hund. Die Eltern klassifizieren für die Kinder das was sie sehen und das Kind speichert es ab. Wenn es dann eine Katze sieht und im ersten Schritt vielleicht sagt, dass ist ein Hund, weil einfach viele Merkmale übereinstimmen, wird es korrigiert und wie auch immer das geht, irgendwann führt es dazu, dass ein Kind eine Katze von einem Hund unterscheiden kann. Und die Entwickler müssen am Ende auch gar nicht verstehen wie das Modell innen wirklich aussieht. Es wird trainiert, getestet und wenn es brauchbare Ergebnisse liefert wird das Modell ausgeliefert. Problem daran ist, dass wir gar nicht genau wissen was ein Modell tut und somit auch nicht voraussagen können, was dieses als Output liefert. Umso mehr Daten man hat umso besser wird die Voraussage. 

Beim Training werden also bekannte Daten in ein Neuronales Netzwerk gegeben und sich angesehen ob das Ergebnis des Netzwerks stimmt. Wenn es nicht stimmt werden die Gewichtungen mit einer mathematischen Formel angepasst. Das macht man für sehr viele Daten. Bei großen Datensätzen kann das trainieren sehr lange dauern und sehr rechenintensiv sein. Da die Berechnung von der Mathematik der von 3D Grafik ähneln, werden oft auf Grafikkarten zurückgegriffen um das Trainieren schnell hinzubekommen. Das Modell dass deployed wird, dauert dazu im Gegensatz nicht wirklich lange. Hier kann es vorkommen, dass man bei einer schnellen Reaktionszeit auch auf GPUs setzt. Wenn man z.B. in einer Produktion die Qualität eines Produkts über Bildverarbeitung mit KI setzt und man sehr viele Teile produziert, ist Geschwindigkeit auch hier wichtig.

Mit einem neuronalen Netz kann man nicht nur Hunde und Katzen erkennen. Man erkennt Ähnlichkeiten in Bilder, man kann Alter, Geschlecht oder Stimmung von Personen erkennen. Man kann sich mit dem Thema predictive Maintenance beschäftigen, also der Voraussage wann ein Produkt repariert werden muss bevor ein größerer Schaden auftritt. Man kann den Musikgeschmack ermitteln und ähnliche Titel vorschlagen. Auch Liveübersetzungen sind möglich oder das Generieren von Texten die man nicht mehr von menschlichen Texten unterscheiden kann. Oder man kann Weltklassespieler in Schach oder GO schlagen

Die Möglichkeiten sind wirklich sehr groß und können Ihre Digitalisierung weit voranbringen. Aber das Trainieren von Modellen ist großer Aufwand. Daten bekommen, vorab klassifizieren und dann noch testen wie gut das Modell ist, ist wirklich Aufwand kostet Zeit und Geld und schwer vorauszusehen ob man einen Mehrwert generiert. Ein etwas schwieriger vielleicht sogar ethischer Punkt ist, dass Neuronale Netzwerke immer in Kategorien „denken“. Das sieht man z.B. bei sozialen Netzwerken wo man Themen vorgeschlagen wird die seiner eigenen Meinung entsprechen. Der Mensch wird über Likes kategorisiert und am Ende bekommt er nur noch Beiträge von anderen Menschen die der gleichen Kategorie entsprechen. Oder bei Musikabos wo man immer nur Musik vorgeschlagen bekommt die der ähnlich sind die man schon gehört und geliked hat. Andere Musik bekommt man schwer zu sehen außer man sucht explizit nach dem Namen. Ein neuronales Netz kann nicht außerhalb der Kategorien „denken“ und so verliert man das individuelle das ein Thema vielleicht ausmachen kann.

Von der Idee zur Umsetzung

Jedes Digitalisierungsprojekt startet mit einer Idee. Startups versuchen aktuelle Branchen mit disruptiven Ideen zu übernehmen. Amazon hat den Handel übernommen, Booking.com und AirBnB das Hotelgewerbe, Uber das Taxigewerbe und Spotify die Musikbranche. Diese Unternehmen haben sich eine Branche ausgesucht und übernommen, durch die Möglichkeiten der Digitalisierung. Alle diese Firmen sind Softwareentwicklungsunternehmen.

Eine Idee kann eine neue weltbewegende disruptive Innovation sein oder auch nur das Digitalisieren eines kleinen Prozesses der in Ihrem Unternehmen noch manuell durchgeführt wird. Z.B. anstatt Rechnungen in Excel oder Word zu schreiben, kaufen Sie sich eine Rechnungschreibungssoftware. Oder Sie können sich überlegen wie Sie Ihre Produkte um digitale Services ergänzen. Wenn Sie gar keine Idee haben, schauen Sie welche Prozesse in Ihrem Unternehmen mit Excel abgebildet sind. Das ist immer ein guter Ansatzpunkt für Digitalisierung. Wichtig ist, dass Sie sich nicht übernehmen. Suchen Sie sich am Anfang ein kleines Projekt heraus, dass Ihnen die Schwachstellen in Ihrem Unternehmen aufzeigt die die Digitalisierung verhindert:

- Haben Sie die richtige Qualifizierung Ihrer Mitarbeiter
- Sind Ihre Mitarbeiter und Leiter gewillt sich zu verändern 
- Gibt es IT Restriktionen die es Software Entwicklern schwer macht Themen einfach und effektiv umzusetzen.
- Ist Ihre HR in der Lage schnell und unkompliziert gute IT-Experten einzustellen. Die sind schneller weg als sie schauen. 

Schauen Sie sich das erste kleine Projekt an und lernen Sie aus dessen Fehlern. Und erst wenn Sie diese Probleme ausgemerzt haben, machen Sie ein nächstes Projekt.

Eine wichtige Lektion in der Digitalisierung, denken Sie immer in Schritten. Fangen Sie klein an und wenn Sie einen ersten Schritt erfolgreich abgeschlossen haben, gehen Sie den nächsten. Das muss ein Mantra werden in jedem Digitalisierungsprojekt.

Idee konkretisieren

Wenn Sie eine Idee haben welches Thema oder welchen Prozess sie digitalisieren wollen, versuchen Sie diese Idee zu konkretisieren. Seien Sie vor allem im Ergebnis offen. Vielleicht ist der aktuelle Prozess suboptimal, vielleicht haben Ihre Mitarbeiter gute Ideen das Thema noch einfacher und besser zu machen, vielleicht wird der Prozess gar nicht mehr benötigt. Das Nutzen von kreativen Methoden ist sinnvoll. Von einfachen „drüber-nachdenken“, Brainstorming, Reizwortanalyse bis hin zu einem Design Thinking Workshop ist alles möglich. 

Bei ersten Projekten kann es sein, dass ein Business Value zu rechnen keinen Sinn macht. Sie müssen Wissen aufbauen, sie werden Fehler machen und müssen aus diesen lernen. Das dauert Zeit und kostet Geld. Vor allem sollten Sie immer überlegen ob das Entwickeln von eigener Software notwendig ist. Wenn man z.B. eine Rechnungsschreibung oder ein Projektmanagementtool einführen möchte, weil man vorher mit Word oder Excel geplant und abgerechnet hat, macht es Sinn sich diese von einem Hersteller zu kaufen.

Eine eigene Software wie eine App oder eine Webseite zu entwickeln oder das Sammeln und Analysieren von Daten kann trotzdem Sinn machen da Unternehmen oft spezielle Prozesse haben die von Standardsoftware nicht abgebildet werden. Allerdings werden Sie diese Software so schnell nicht wieder los. Sie müssen die Software entwickeln und betreiben. Sie benötigen Support falls es Probleme gibt und Sie werden neue Ideen und Anforderungen nachträglich einbauen. Seien Sie sich darüber im Klaren, dass Software Projekte auch scheitern können und Sie viel Geld investieren bei dem Sie keinen Mehrwert generieren. 

Wenn man jetzt eine eigene Software schreiben möchte, weil es keine Lösung am Markt gibt oder weil diese nicht auf die aktuellen Prozesse passen, müssen auf alle Fälle folgende Punkte betrachtet werden auf die wir alle näher eingehen werden:

- Erfassen der Anforderungen
- Zusammenstellen eines Teams, intern oder extern
- Entwickeln der App auf Basis der Anforderungen
- Rollout oder Deployment
- Betrieb mit Bugfixing und Hinzufügen neuer Anforderungen

Ein Softwareprojekt hat selten ein konkretes Ende. Wenn die Hauptanforderungen umgesetzt sind und die Benutzer damit arbeiten findet man immer noch mehr neue Anforderungen die man umgesetzt haben möchte oder Bugs die zu beheben sind. Außerdem kommt es vor, dass man Benutzer schulen muss oder manchmal wird eine Technologie obsolet die man durch eine neue ersetzen muss. Planen Sie die Themen Weiterentwicklung, Betrieb und Support mit ein. Man redet hier auch von dem Application Lifecycle Management (ALM)  welches 5 Phasen umfasst, 

 

**Konzeptphase**	 **- Realisierungsphase – Qualitätsphase – Rolloutphase – Wartungsphase** 

Das ALM unterscheidet sich je nach Vorgehensweise des Projektmanagements. Wir reden hier von zwei Vorgehen: Wasserfallmodell oder Agilität. Während das Wasserfallmodell die Software von Anfang bis Ende durchdenkt, arbeitet man sich in agilen Prozessen iterativ durch die einzelnen Phasen. 

Projekt Management

Startet man ein Software Projekt/Produkt muss man sich darüber im Klaren sein welches Projektmanagementvorgehen man wählt. In den letzten 10 Jahren hat sich die agile 

Vorgehensweise sehr stark durchgesetzt.

Wasserfallmodell

Konzept - Realisierung – Qualität – Rollout – Wartung

Agil



Man sollte kein Projekt mehr mit Wasserfallmodell entwickeln. Es hat sich einfach herauskristallisiert, dass in der Softwareentwicklung das Wasserfallmodell in vielen Fällen nicht zum Erfolg führt. 

Software Entwicklung ist ein sehr schwieriges Feld und es ist unmöglich vorherzusehen wie sich das Projekt entwickeln wird. Die Chaos Studie  veröffentlicht regelmäßig wie viel Prozent der Softwareprojekte erfolgreich sind, teilweise erfolgreich und wie viele abgebrochen wurden. Die teilweise erfolgreichen Projekte machen meistens die Hälfte aller Projekte aus. D.h. selbst bei einer gründlichen Planung verlaufen die meisten Projekte anders als man voraussagen kann. Deswegen ist es besser nicht zu viel Zeit in eine anfängliche Planungsphase zu verlieren, sondern mit wenigen Anforderungen zu starten und iterativ zu verbessern. Dies erfordert mehr Change als man vermuten mag. Ihre Projektleiter müssen in Schritten denken, die Kunden ständig mit einbeziehen und auch mal bereit zu sein das Projekt komplett auf den Kopf zu stellen. Es ist schwer die Kosten für ein agiles Projekt vorherzusehen. Am Ende ist es aber das Ziel das Geld das man investiert in das Richtige zu investieren. Das Management muss sehr viel Vertrauen in den Projektleiter haben und kann nicht erwarten, dass es einen genauen Plan der Entwicklung gibt. Die Vision des Produkts muss sauber definiert sein an dem sich der Projektleiter ausrichten kann. Steuerkreise und Meilensteine sind kontraproduktiv da sie die Entwicklung eher verlangsamen und dem Projektleiter die „Bewegungsfreiheit“ einschränken. Das heißt aber nicht, dass der Projektleiter einen Freibrief bekommt. Das Management darf sich jederzeit über den Fortschritt des Projektes informieren und auch ein Projekt stoppen, wenn es in die falsche Richtung läuft.

In der Software Entwicklung gibt es den Begriff des Pivotisierens. D.h. dass man eine Idee mitten drin vollkommen auf den Kopf stellt, weil man feststellt, dass die anfängliche Vision so nicht umgesetzt werden kann oder keinen Sinn macht. Das ist in der Entwicklung vollkommen normal, es sollte Sie eher verwundern, wenn ein Projekt nicht pivotisiert wurde.

Agiles Projekt Management

Agilität ist einer der Begriffe die in den letzten Jahren sehr häufig mit Erfolg in Unternehmen in der Zukunft in Beziehung steht. Die Begrifflichkeit ist schwer zu fassen. Darüber gibt es einen Haufen Bücher, Blogs, Diskussionen die man sich durchlesen kann. Man kann Agilität gut mit einem Ball - Teamsport vergleichen. Z.B. Fußball. Eine Mannschaft versucht zusammen ein Ziel zu erreichen im Normalfall der Gewinn des Spiels. 

Jeder Spieler agiert in seiner zugewiesenen Rolle selbstständig und versucht sein bestes damit das Team gewinnt. Der Trainer steht an der Seite, kann nur bedingt in das aktuelle Spiel eingreifen, das Management ist nur Zuschauer. Der Trainer hofft, dass er die Spieler richtig auf das Spiel vorbereitet hat. Nach dem Spiel wird das Spiel analysiert, ein Trainingsplan erstellt, eine Taktik entwickelt und versucht die Fehler die in dem Spiel passiert sind im nächsten Spiel zu vermeiden und sich kontinuierlich zu verbessern. Eine hierarchische Struktur wie sie in Unternehmen häufig zu finden sind könnte bei solchen sich schnell ändernden Prozessen wie bei einem Fußballspiel nicht funktionieren. Ein Stürmer kann ja nicht den Trainer fragen ob er den Ball wirklich ins Tor schießen soll, wie ein Mitarbeiter den Chef fragt ob er jetzt eine Bestellung freigeben darf.







Das Problem bei Fußball ist, dass die Situation sich von einem Moment auf den nächsten komplett ändern kann und das Team muss ohne Steuerkreis, Workshops und Besprechungen auf die Situation reagieren. Jedes Teammitglied muss ständig die aktuelle Situation im Blick behalten und entsprechend selbstständig darauf reagieren und auch mal die Rolle ändern, wenn es nötig ist. Z.B. ein Stürmer übernimmt die Verteidigung einer gefährlichen Szene, wenn es notwendig wird. 

Bei Fußball passieren auch ständig Fehler. Der Ball will nicht richtig gestoppt, ein Pass landet beim Gegner, die Positionierung eines Mitspielers ist falsch. Trotzdem hört man selten, dass sich die Teammitglieder gegenseitig Schuld zuweisen. Die Fehler werden akzeptiert und im Nachgang aufgearbeitet. Würde man hypothetisch eine Fußballmannschaft wie wir es aktuell kennen gegen eine hierarchisch geführte Fußballmannschaft spielen lassen, welches jeden nächsten Schritt erst mal mit dem Trainer abstimmen muss, wer würde hier gewinnen?

Dieses Verhalten könnte man als Agilität deuten. Stellen Sie sich Ihr Unternehmen vor welches sich von selbst auf neue Herausforderungen einstellt, in dem sich die Abteilungen neu positionieren, ohne dass die Hierarchie durchgreifen muss, ohne Eskalationsstufen, ohne endlose Workshops und Umorganisationen, ohne sich gegenseitig die Schuld zuzuweisen, ohne Projektleiter die ihr Projekt positiver darstellen als sie wirklich laufen und ohne Abteilungen die zu fest an Prozessen festzuhalten auch wenn Sie in manchen Fällen keinen Sinn ergeben und ohne Steuerkreise die nur alle Monate treffen und sich somit dringend benötigte Entscheidungen verzögern. Sondern Abteilungen die sich gegenseitig unterstützen, wenn Not am Mann und Fehlern positiv gegenüberstehen und sich ständig weiter verbessern zu wollen um die Vision Ihres Unternehmens umzusetzen.

Gerade jetzt wo disruptive Innovation von kleinen Firmen ganze Geschäftsbereiche bedrohen, muss man schnell reagieren können. Es gibt viele Beispiele bei denen Firmen nicht in der Lage waren auf eine disruptive Innovation zu reagieren obwohl es viele Jahre Zeit gedauert hat bis das Geschäftsmodell kaputt ging, Kodak oder Quelle werden immer wieder als Beispiele genannt. Sich diese Geschichten einmal näher anzusehen ist auf alle Fälle immer interessant um den Sinn von Agilität zu verstehen. Aber vielleicht können Sie erahnen welcher Change das für ein Unternehmen bedeuten kann?

Agilität bedeutet auch immer etwas Chaos. Man kann bei einem Teamsport auch nie vorhersagen was als nächstes passiert, es gibt Regeln und Taktiken die aber relativ viel Freiraum bieten. Bei Agilität gibt es weniger Prozesse an die man sich halten kann. Feste Prozesse geben Sicherheit, nehmen dem Mitarbeiter aber auch die Reaktionsfähigkeit, wenn neue Herausforderungen auf das Unternehmen einprasseln. Es wird mehr situativ entschieden, mehr versucht den gesunden Menschenverstand einzusetzen und das Ganze mit Know How der Mitarbeiter zu unterfüttern. Die Frage steht immer im Raum, halte ich mich an einen Prozess auch wenn es in dem speziellen Fall keinen Sinn ergibt? Als wichtigstes Instrument ist die Vision, das Ziel des Unternehmens. Das Management gibt die Vision, Mission und Strategie vor. Diese zu ändern muss für Abteilungen ausreichen damit diese sich neu ausrichten. 

Aber man versucht auch mit leichten Prozessen etwas Ordnung ins Chaos zu bringen. Dazu kann man auf das Vorgehensmodell von Scrum, Kanban, Extreme Programming in Teams und in abteilungsübergreifenden Projekten auf Less, Safe oder andere Modelle zugreifen. 

Die Stacey Matrix kann einem bei der Auswahl helfen:

Diese Matrix ist so zu lesen. Umso weiter sie weg vom 0 Punkt sich entfernen umso weniger Ahnung haben sie bei den Anforderungen (Y-Achse) oder bei der Technik (X-Achse). D.h. wenn Sie überhaupt keine Ahnung davon haben was Sie umsetzen möchten (Anforderungen) und wie es umzusetzen ist (Technik), landet Ihr Punkt im Chaos und sollten vielleicht einen Proof of Concept erstellen und mehr versuchen Ihre Anforderungen zu konkretisieren. Wenn Sie ganz nah am Null-Punkt sind, benötigen Sie keine Agilität. Wenn die Technik und die Anforderung vollkommen klar sind dann fangen Sie nicht mit agilen Prozessen an, dann arbeiten Sie im Wasserfallmodell. Das ist z.B. beim Support der Fall. Es kommt ein Supportfall herein, der wird abgearbeitet fertig.

Das Ziel das Sie verfolgen sollten ist, dass man über die Dauer eines Projektes immer näher an den Nullpunkt kommt. So kann es durchaus vorkommen, dass am Anfang eines Projektes erst Workshops und einen Proof of Concept (POC) anfangen, dann mit Scrum in eine konkrete Umsetzung starten und man dann auf einer bestimmten Stufe Kanban wechselt. Es wird wohl nicht so oft passieren, dass ein Software Entwicklungsprojekt von den Anforderungen und der Technik so klar ist, dass man mit Wasserfall arbeiten sollte, aber Kanban sollte Ihr Ziel sein. Wir werden später erklären warum.

Kurze Einführung in agile Prozesse

Scrum basiert auf Sprints. In zwei (kann variieren) Wochen Sprints werden neue Features entwickelt. Die Länge der Sprints sind fix und werden nicht geändert. Alle zwei Wochen dürfen sich die Features ändern. Man plant im Idealfall natürlich etwas weiter voraus und hat eine Vision des Produkts. Der Kunde wird, soweit bekannt, sehr stark in den Entwicklungsprozess eingebunden. Dafür gibt es eine dedizierte Rolle, den Product Owner. Der ist für den Kundenkontakt und für die Formulierung der Anforderung zuständig, die sogenannten User Stories. Diese werden priorisiert und an das Team weitergegeben, immer am Sprintanfang. Am Sprintende werden die Ergebnisse präsentiert und an den Kunden weitergegeben. Eine wichtige Rolle kommt dem Scrum Master zu. Dieser ist prinzipiell dazu da, dass der Scrum Prozess eingehalten wird, dass Probleme und Fehler erkannt werden und dass das Team sich weiter verbessert.

Kanban besteht im Prinzip wie in der Produktion aus einem Board mit den Arbeitsschritten und die Anzahl wie viele Aufgaben maximal in dem Arbeitsschritt gleichzeitig ausgeführt werden dürfen. Das führt von einem Push zu einem Pull Prinzip und dazu, dass man herausfindet wo die Engpässe sind. 

Auch wenn es keine dedizierten Rollendefinitionen gibt sollte man zumindest einen Mitarbeiter die Aufgabe geben die Kundenanforderungen aufzunehmen und einen agilen Coach beschäftigen. Der Vorteil von Kanban ist, dass die Arbeitsergebnisse direkt nach Abschluss sofort zur Verfügung gestellt werden und man nicht auf ein Sprintende wartet.

Agilität und Agile Prozesse unterscheiden sich sehr stark von den aktuell bekannten und gewohnten Vorgehensweisen. Ein Chef der das Team führt, der Ziele definiert, Entscheidungen trifft, neue Mitarbeiter einstellt und der Herr über die Gehälter ist, gibt es hier nicht. Man teilt die Aufgaben des Chefs Mitarbeiterführung, fachlich Verantwortlicher und technischer Entscheidungsträger auf. Mitarbeiterführung übernimmt der agile Coach, fachlich Verantwortlich wird der Product Owner und technische Entscheidungen trifft das Umsetzungsteam. Wenn Sie zu dem Chef noch einen Product Owner und agile Coach beschäftigen führt zu einem Machtüberhang, wenn der Chef und der agile Coach gleichzeitig versuchen die Mitarbeiter zu führen und ihnen Feedback geben. Es kommt manchmal auch zu einem Machtvakuum, wenn der Chef sich komplett rauszieht der agile Coach aber gar nicht den Einfluss auf die Mitarbeiter haben kann den er bräuchte, weil der Chef das Gehalt erhöhen kann. Deswegen muss man hier eine gute Regelung finden, z.B. könnte der Chef der agile Coach werden oder ein Chef übernimmt mehrere Teams. Auf dieses Problem sollten sie großes Augenmerk legen.

Es gibt noch einen anderen Grund Agilität einzuführen. Gute Software Entwickler sind oft Menschen die aus intrinsischer Motiviation Software programmieren (Ich persönlich habe mit 12 Jahren das erste Mal versucht freiwillig Programme zu schreiben). Geld steht oft nicht so im Vordergrund eher der Reiz an der Technologie und die Herausforderung Probleme zu lösen. Es sind aber auch oft kreativ denkende Menschen die einen gewissen Freiraum benötigen. Agile Prozesse nehmen sehr gut auf die Bedürfnisse von Software Entwicklern Rücksicht. Man sollte sich auch damit auseinandersetzen, dass gute Software Entwickler nicht immer von Haus aus gute Softskills mitbringen und dass es einen fähigen agilen Coach benötigt der mit diesen Leuten umgehen kann.

Der Umstieg der Software Entwicklung auf agile Vorgehensmodelle kann in bisher hierarchisch geführten Unternehmen ziemlich daneben gehen. Man sollte schon laufende Projekte nicht unbedingt auf Scrum oder Kanban umstellen. Das kann zu einer größeren Verzögerung des Projektes führen. Allerdings ist davon auszugehen, dass Unternehmen die nicht auf Agilität zumindest in der Software Entwicklung setzen, in Zukunft einen sehr großen Wettbewerbsnachteil erleiden. Wieder ein Grund mehr klein anzufangen und nicht gleich das ganze Unternehmen auf Agile umzustellen. Fangen Sie mit einem Team an und begleiten Sie das Team sehr intensiv wie gut der Umstieg funktioniert. Räumen Sie Hemmnisse beiseite, dann können weitere Teams umstellen. Suchen Sie sich Leute die Lust auf eine Veränderung haben und achten Sie darauf, dass es keine Störenfriede gibt. 

Wenn Ihre Mitarbeiter seit Jahren oder Jahrzehnten gewohnt sind das umzusetzen was ihnen der Chef sagt wird der Umstieg auf Agile womöglich sehr schwerfallen.

Lean software development

Bevor wir langsam in die Technik einsteigen noch ein paar Worte zu Lean. Lean kommt aus der Produktion und besteht aus mehreren Prinzipien die auch Überschneidungen mit der Agilität haben. Diese Prinzipien werden uns immer wieder begegnen, hier die wichtigen für die Software Entwicklung:

Verschwendung vermeiden

Man sollte in seinem Projekt nur die Features umsetzen die man auch wirklich braucht. Keine unnötigen Features, keine zu weit vorrauschauenden Planungen was vielleicht kommen könnte. Kleine Schritte um zu sehen wie die Kunden reagieren. Keiner kann in die Zukunft schauen und Software Entwicklung ist oft die Suche im Nebel den richtigen Weg durchs Moor zu finden.

Lernen unterstützen

Mitarbeiter sollten einiges an Zeit investieren können um auf den neuesten Stand zu bleiben. Eine Auslastung von maximal 80% für das Projekt ist anzustreben. Die restliche Zeit können Entwickler sich mit neuen Themen beschäftigen. Stellen Sie sich eine 100% Auslastung der Autobahn vor, das bedeutet normalerweise Stau oder stockender Verkehr. Außerdem verlieren die Mitarbeiter relativ leicht den Anschluss an technologische Entwicklungen. Vermeiden Sie es unbedingt Mitarbeiter in Prozenten auf verschieden Themen aufzuteilen. Verschiedene Projekte mit unterschiedlichen Anforderungen im Blick zu behalten ist sehr schwer. Dazu noch der Druck durch mehrere Projektleiter ihre Arbeit zu bevorzugen ist für kreative Arbeit nicht förderlich.

So spät entscheiden wie möglich

Wie man ein Feature umsetzt sollte natürlich dann entschieden werden, wenn das Feature umgesetzt wird. Auch die Technologie die eingesetzt wird sollte nicht schon im vornherein feststehen da man damit Bedingungen setzt die es später unnötig erschweren Features umzusetzen.

So früh ausliefern wie möglich

Ein sehr zentrales Element in der Software Entwicklung. Hier fällt oft das Wort MVP, das Minimum Viable Product. Eine Idee die Sie haben ist oft auch erst mal eine These die es zu überprüfen gilt. Oft weiß man gar nicht ob die Idee von Nutzern angenommen wird und muss erst einmal überprüfen wie gut die Idee beim Kunden ankommt. Um nicht zu viel Geld in eine Idee zu investieren die der Kunde nicht nutzt, muss man so schnell wie möglich herauszufinden ob die Idee gut oder schlecht ist um im Notfall schnell die Reißleine zu ziehen. Deswegen ist es in einem Projekt sehr wichtig sich zu überlegen welche Features sind in einer ersten Version unabdinglich und was kann man nach hinten schieben.

Der Vorteil in der Software Entwicklung ist, dass man per Update neue Features nachliefern kann. Das kann man bei anderen Dingen nicht wie z.B. Hardware. Ich kann dem Kunden keinen Laptop zuschicken der erst mal nur eine kleine Festplatte hat und diese dann bei Bedarf auf Knopfdruck erhöhen.

Wenn Sie ein Auto kaufen und Sie hätten die Wahl das Auto innerhalb eines Monats zu bekommen welches fährt, aber der Radio und die Klimaanlage würde in den nächsten zwei Monaten per Knopfdruck über Nacht nachgeliefert oder Sie müssen für das komplette Auto drei Monate warten, was von den beiden Kaufmodellen würden Sie wählen?

Bei der Software ist das eine Möglichkeit schnell einen Mehrwert für den Kunden zu generieren und dann nach und nach die neuen Anforderungen nachzuliefern. Deswegen wird Software immer Iterativ entwickelt. Wichtige Anforderungen zuerst entwickeln, ausliefern, Feedback einholen, nächste Anforderungen entwickeln, ausliefern, Feedback einholen. Das bewahrt die Teams vor einer langen Entwicklungszeit ohne Mehrwert und kann dabei helfen das Projekt, wenn es falsch verläuft auch frühzeitig zu stoppen und Kosten zu reduzieren. Um einen MVP zu definieren helfen Moderationswerkzeuge wie z.B. Userstory Mapping 

Verantwortung an das [Team](https://slack.com/intl/de-de) geben

Das Team besteht aus den Experten die am besten entscheiden können wie ein Thema umgesetzt wird. Software Entwicklung ist ein sehr komplexer Prozess bei dem viele Themen eine Rolle spielen und ständig neue Technologien auf dem Markt erscheinen. Hier einen Überblick zu behalten ist schon schwer genug, aber jede Technologie zu verstehen ist nicht möglich. Deswegen muss derjenige der das Wissen hat entscheiden dürfen wie etwas umgesetzt wird. Damit das Team aber die Verantwortung übernehmen will muss es im Unternehmen erlaubt sein, Fehler zu machen ohne Repressalien zu fürchten. Außerdem muss dem Techniker Zeit gegeben werden sich mit den neuen Technologien auseinanderzusetzen. Und seien wir ehrlich, ein Steuerkreis bekommt am Ende drei Vorschläge und eine Empfehlung und entscheidet, wenn er intelligent ist nach der Empfehlung. Dann kann man sich diesen auch sparen.

Kundenorientierung

Der Kunde muss von Anfang an mit einbezogen werden. Wenn man einen konkreten Kunden hat der die Entwicklung bezahlt ist das einfach. Wenn man diesen nicht hat, kann man mit Personas arbeiten bzw. auch Umfragen starten. Umso wichtiger ist es aber auch sich schnell Feedback vom Markt zu holen in dem man schnell einen MVP ausrollt.

Team

Nachdem Software Entwicklung ein komplexer Vorgang ist baut man normalerweise Teams auf mit verschiedenen Spezialisten auf einem Gebiet um eine Software in allen Facetten umsetzen zu können. Bei größeren Anwendungen baut man mehrere Teams auf. Je nach Typ der Anwendung benötigt man in den Teams unterschiedliche Spezialisten. 

- Product Owner
1. Legt die Vision und die Anforderungen fest und verantwortet somit auch wie gut das Produkt beim Kunden ankommt.
1. 
- Software Architekt
1. Legt die Softwarearchitektur fest, ist erster Ansprechpartner für den PO für technische Fragen und entwickelt trotzdem Features.
1. 
- Entwickler 
1. Die Entwickler schreiben den Code. Welche Technologien diese können müssen hängt von dem Projekt ab.
1. 
- Tester
1. Tester müssen sich gut mit Testing auskennen. Sie müssen Testpläne schreiben können und sich auch mit der Testautomatisierung auskennen.
1. 
- Deployer (Devops)
1. Der Deployer kümmert sich um das Ausrollen und Monitoring der Software. Er ist für alle Schritte bis dahin verantwortlich und dafür, dass die meisten Aufgaben automatisiert ablaufen.

DevOps

DevOps ist ein Wort das in letzter Zeit vor allem für Rolle eines Mitarbeiters verwendet wird, der den Rollout der Software und den Betrieb übernimmt. DevOps ist aber eher eine Idee, ein Mindset das sehr stark den Lean Gedanken in die Software Entwicklung bringt. Früher war es so, es gab Teams die haben entwickelt und es gab Teams die mussten die Software ausrollen und supporten. Diese waren getrennt was immer wieder zu Problemen geführt hat. Die Entwickler haben sich nicht wirklich dafür verantwortlich gefühlt, dass die Software richtig läuft und die Supporter haben die Schuld, wenn etwas nicht lief den Entwickler gegeben. Dazu kam dass man Übergaben benötigt hat und Dokumentation, damit Supporter wussten wie sie die Software ausrollen mussten. Deswegen hat man irgendwann angefangen die Entwickler (Developer also Dev) und die Supporter (Operations also Ops) zusammen in ein Team gesteckt und gesagt: Ihr seid für das komplette Produkt verantwortlich. Also DevOps, ein Team das vom Requirement Engineering bis zum Betrieb und Wartung alles macht.

Fehler passieren

Sie sollten sich in Ihrem Unternehmen überlegen wie sie mit Fehlern umgehen. Fehler passieren in der Software Entwicklung, da kann man nichts dagegen tun. Die Software Entwicklungscommunity hat schon viele Ideen wie man die Qualität der Entwicklung erhöht. Diese Methoden wie Testautomatisierung sollten unbedingt eingesetzt werden. Nichts destotrotz sollten Sie den Teams die Angst nehmen Fehler zu machen. Es gibt Firmen die feiern z.B. eine Worst–Failure-Party wo Teams ihre schlimmsten Fehler präsentieren und die Lösung, dass diese nicht wieder auftreten. Das wichtige daran ist es, dass Fehler offen ausgesprochen werden, damit man eine Lösung finden kann und Strategien, dass solche Fehler nicht mehr passieren. Es gibt nichts Schlimmeres, als dass Fehler verschwiegen werden. Umso schneller Fehler kommuniziert und behoben werden umso weniger Kosten entstehen. Eine Strategie wie diese Fehler in Zukunft vermieden werden sollte ausgearbeitet werden. Was nicht passieren sollte ist, dass ein und derselbe Fehler immer wieder auftritt.

Agilität bedeutet nicht, dass der Druck auf die Projekte und die Teams nicht hoch sein kann. Auch hier kann man Fußball als Beispiel bemühen. Wenn man in der Bundesliga spielen möchte braucht man sehr gute Fussballspieler, die selten sind und die hoch bezahlt werden. Wenn ein Spieler von Spiel zu Spiel keine Leistung zeigt wird er ausgemustert. D.h. man darf Agilität nicht mit Kuschelkurs verwechseln. Der Druck kann hier viel höher auf den Mitarbeitern lasten da sie selber Entscheidungen treffen müssen. Das liegt nicht jedem. Man braucht für eine erfolgreiche Agilität gute Mitarbeiter die motiviert sind, flexibel auf Herausforderungen reagieren, die Verantwortung übernehmen und die wissen was sie tun. Diesen Mitarbeitern passieren auch genügend oft Fehler, aber auf einem anderen Niveau mit einer anderen Lernkurve. 

Heutzutage haben die Arbeitgeber das Problem, dass gute IT-Fachkräfte schwer zu finden bzw. diese sehr begehrt sind und oft ist ein Gehalt nicht das ausschlaggebende Argument ist für einen Arbeitgeberwechsel. Hier müssen Sie sich überlegen wie Sie gute Mitarbeiter an ihr Unternehmen binden. Eine gute Idee kann schon einmal sein, wenn Sie für IT-Fachkräfte Ihre Einstellungsprozesse anpassen um auch mal schnell einen guten „Fang“ machen zu können.

Requirements Engineering

Es ist eine Sache eine Idee für die Digitalisierung zu haben und eine andere die Branche zu verstehen und eine Anwendung zu schreiben die von den Nutzern akzeptiert wird und sich gegen Konkurrenz durchsetzen kann.

Das Requirements Engineering ist unheimlich wichtig, deswegen gibt es dafür im Idealfall eine eigene Rolle die sich dieser Aufgabe widmet. Der Product Owner kümmert sich darum, dass die richtigen Anforderungen in das Produkt kommen und trägt somit auch die Verantwortung für den Erfolg des Produkts. Der Begriff des Product Owners kommt aus dem Prozess von Scrum kann aber auch in anderen Vorgehensmodellen eingesetzt werden. Requirements bzw. Features oder auch Anforderungen werden auf fachlicher Basis aus Sicht des Nutzers erfasst, sogenannte User Stories. Eine User Story besteht aus: 

**Welcher Nutzer möchte was und warum.** 

Das sind drei wichtige Fragen um zu klären, dass die Anforderung auch wirklich wichtig ist und seinen Grund hat. Wenn man das Wer oder das Warum nicht beantworten kann ist die Anforderung vielleicht gar nicht so wichtig. 

Ein wesentlicher Bestandteil sind die **Akzeptanzkriterien**, also die Fragen wann gilt die Anforderung als umgesetzt. Eine Liste von Kriterien die erfüllt sein müssen. Das hilft unheimlich im Verständnis der Anforderung. Die Entwickler sollten so genau wie möglich wissen was die User Story beinhaltet und wenig Interpretationsspielraum haben. Interpretationen führen zu Verschwendung von Entwicklungsressourcen. Die User Stories werden in ein Product Backlog abgelegt einer priorisierten Liste von User Stories und vom Team von oben nach unten abgearbeitet. Nur die obersten User Stories müssen im Detail vorliegen. Es kann auch viele User Stories geben die erst mal nur eine Überschrift beinhaltet um eine Idee oder eine Anforderung an spätere Versionen zu dokumentieren. In der Formulierung von User Stories ist ein Konjunktiv unbedingt zu vermeiden, damit hier keine Missverständnisse auftreten.

User Stories werden in manchen Teams geschätzt. Diese Schätzung basiert nicht mehr auf echten Zeiteinheiten sondern auf einer fiktiven Einheit, sogenannte Story Points die der Fibonacci Reihe entsprechen. Also 1, 2, 3, 5, 8, 13, 21. Man schätzt die Zeit auf Basis der User Stories nicht mehr absolut in Zeit, sondern relativ im Verhältnis zwischen den Anforderungen. Der Mensch ist scheinbar besser darin relative Schätzungen abzugeben. Wie groß ist eine Ameise und wie groß ist ein Elefant ist schwieriger als die Frage was ist größer eine Ameise oder ein Elefant. Die Summe der Story Points die das Team in einem Sprint schafft pendelt sich nach einer gewissen Zeit auf einen stabilen Wert ein so dass man nach ein paar Sprints relativ genaue Voraussagen machen kann wann welche Anforderung umgesetzt ist und wie lange ein Projekt dauert. Schätzungen auf konkrete Zeit hat sich in den letzten Jahrzehnten immer wieder als sehr fehlerhaft erwiesen.

Die Priorisierung des Produkt Backlog kann sich als schwieriges Optimierungsproblem herausstellen. Welches der Userstories als nächstes gemacht werden hängt dabei von vielen Faktoren ab:

- Nutzen für den Kunden
- Aufwand
- Kundenwunsch
- Abhängig vom Teamaufbau
- Abhängigkeiten zu anderen Teams
1. 

Eine User Story die schnell umgesetzt ist und viel Nutzen für den Kunden bringt ist natürlich eher zu Priorisieren als eine US die lange dauert und wenig Nutzen bringt. Aber es kann auch sein, dass man zwei Userstories nicht in einem Sprint erledigen kann, weil sonst zu viele Entwickler auf den gleichen Dateien arbeiten sie sich gegenseitig blockieren. Eine falsche Priorisierung der User Stories kann zur Verzögerung des Projektes führen.

Funktionale und nicht funktionale Anforderungen

Man unterscheidet bei Anforderungen zwischen funktionale und nicht funktionale Anforderungen. Um das zu verdeutlichen versuchen wir das mit einem Haus zu vergleichen. Ein Hausbauer hat an sein Haus die funktionale Anforderung, er möchte darin wohnen, kochen, duschen, schlafen, manch einer möchte Basteln oder einen großen Raum für eine Eisenbahn oder jeden Tag im Pool schwimmen. Das sind konkrete Dinge die ich in meinem Haus tun möchte. Dazu benötigt man eine Küche, ein Sofa, ein Bett, vielleicht einen Pool. Die nicht funktionalen Anforderungen sagen etwas über den Komfort aus. Ich kann in einem kleinen kalten Pool schwimmen oder in einem beheizten mit Gegenstromanlage.

So verhält es sich auch bei der Software: Die Anforderung: „Ich möchte eine Tabelle mit den Aufträgen eines Kunden angezeigt bekommen“ ist leicht umzusetzen. Aber wer sagt denn wie lange sie auf das Ergebnis warten möchten. Wenn Sie in einem globalen Unternehmen arbeiten mit Hundertausenden von Aufträgen ist das gar nicht so leicht die Aufträge in einer schnellen Zeit anzuzeigen. Das wird über die nicht funktionelle Anforderung Performance angegeben. Funktionale Anforderungen zu definieren ist recht einfach aber nicht funktionale Anforderungen bestimmen wieviel Geld sie für das Projekt ausgeben. Wie beim Pool, ein kleiner kalter Pool kostet weniger als ein großer beheizter.

Nicht funktionale Anforderungen teilen sich in:

- Z[uverlässigkeit](https://de.wikipedia.org/wiki/Zuverl%C3%A4ssigkeit_(Technik)) (Systemreife, Wiederherstellbarkeit, Fehlertoleranz)

Hier hört man oft die unbedarfte Aussage, das System muss immer verfügbar sein. Das ist nicht unmöglich aber wird Ihre Kosten in unsagbare Höhen treiben. 99,5% ist so die Grenze ab dem es richtig teuer wird.

- Aussehen und Handhabung ([Look and Feel](https://de.wikipedia.org/wiki/Look_and_Feel))

Das Aussehen kann von schlicht bzw. Kommandozeile bis hin zu einem super Design. Alles eine Frage des Geldes. Software Entwickler sind nicht für gutes Design bekannt. Benötigen Sie ein schickes Design sollten Sie sich einen Designer suchen.

- [Benutzbarkeit](https://de.wikipedia.org/wiki/Gebrauchstauglichkeit_(Produkt)) (Verständlichkeit, Erlernbarkeit, Bedienbarkeit)

Glauben Sie nicht, dass die Benutzbarkeit eines Programms selbstverständlich ist. Dafür gibt es den Beruf des UX – Designers. Und der muss die Verwender sehr intensiv befragen, Tests machen um herauszufinden ob das Programm auch verständlich ist.

- [Leistung](https://de.wikipedia.org/wiki/Leistung_(Informatik)) und [Effizienz](https://de.wikipedia.org/wiki/Effizienz_(Informatik)) (Antwortzeiten, Ressourcenbedarf, Wirtschaftlichkeit)

Wie schnell muss Ihre Anwendung reagieren. Auf welchen Systemen wird sie laufen. Welche Auswertungen benötigt man schnell auf welche kann man länger warten. Gerade bei großen Datenmengen ist hier normalerweise eine Aussage nötig. Das filtern einer sehr großen Datenmenge dauert und der Entwickler muss sich überlegen wie er das gut hinbekommt.

- Sicherheitsanforderungen (Vertraulichkeit, [Informationssicherheit](https://de.wikipedia.org/wiki/Informationssicherheit), Datenintegrität, [Verfügbarkeit](https://de.wikipedia.org/wiki/Verf%C3%BCgbarkeit))

Wer darf welche Daten sehen, welche Rollen und Rechte wird es geben müssen. Das ist ein riesiges Thema welches genau betrachtet werden muss. Unterschätzen Sie die Sicherheit nicht. Wenn Ihr Unternehmen in der Zeitung steht, dass tausende private Nutzerdaten geklaut wurde kann Sie das Ihre Existenz kosten.

- [Korrektheit](https://de.wikipedia.org/wiki/Korrektheit_(Informatik)) (Ergebnisse fehlerfrei)

Die Frage wie schlimm es ist, wenn manche Daten nicht ganz korrekt gespeichert werden. Bei globalen Webanwendungen muss man manchmal auf Grund der Performance auf Korrektheit verzichten. Also ich sehe meine Daten schnell dafür kann es vorkommen, dass ich nicht auf dem aktuellsten Stand bin.

- [Portierbarkeit](https://de.wikipedia.org/wiki/Plattformunabh%C3%A4ngigkeit) und [Übertragbarkeit](https://de.wikipedia.org/wiki/%C3%9Cbertragbarkeit) (Anpassbarkeit, Installierbarkeit, Konformität, Austauschbarkeit)

Auf welcher Hardware bzw. Betriebssystem muss das System laufen. Die meisten Apps werden heutzutage so entwickelt, dass sie auf vielen unterschiedlichen Geräten laufen. Smartphones, Tablets, Mac, Windows Rechnern vielleicht sogar Windows. Deswegen eignen sich hier Webtechnologien sehr gut.

- [Skalierbarkeit](https://de.wikipedia.org/wiki/Skalierbarkeit) (Änderungen des Problemumfangs bewältigen)

Skalierbarkeit ist durch das Internet ein wichtiges Thema geworden. Webanwendungen können plötzlich „durch die Decke gehen“. Wenn Sie darauf nicht vorbereitet sind und Ihr Webshop einen massiven Ansturm nicht verkraftet, gehen Ihnen Aufträge und Kunden verloren.

- [Wartbarkeit](https://de.wikipedia.org/wiki/Wartbarkeit), Änderbarkeit (Analysierbarkeit, Stabilität, Prüfbarkeit, Erweiterbarkeit)

Wie oft wird sich Ihre Software ändern müssen. Bei Software die von Gesetzen abhängen wie Steuersoftware werden Sie häufige Änderungen vornehmen müssen. 

- Betrieb und Umgebungsbedingungen

Wo wird Ihre Software laufen, in der Cloud oder direkt auf dem PC. Sind diese PCs gut ausgestattet, kann man von guten modernen Rechnern ausgehen oder sind es etwas ältere PCs

Rahmenbedingungen

Zu jedem Projekt gibt es Rahmenbedingungen die in die Entwicklung einfließen z.B. wie das Budget, Termine, die Qualität, Anzahl der Mitarbeiter oder Prozesse bei sicherheitskritischer Software die sich auf das Produkt auswirken. Hier gibt es oft einen Konflikt zwischen dem Management und den umsetzenden Teams. Das Management möchte mit wenig Geld ein großes Projekt in kurzer Zeit stemmen, die Entwickler möchte sich die Zeit nehmen hohe Qualität auszuliefern. Hier hat der Product Owner die Aufgabe aus den Requirements und den Rahmenbedingungen den maximalen Mehrwert für den Kunden herauszuholen.

Fazit: 

Schließen wir hier mit Projekt Management Themen ab und gehen in die Technik. Über das Thema gibt es eine Menge Bücher. Beschäftigen Sie sich damit, am Ende dieses Buches ist Ihnen hoffentlich bewusst, dass Agilität mit modernen Software Architekturen und Technologien Hand in Hand gehen und dass sie einen Wettbewerbsnachteil erleiden werden, wenn Agilität und Lean Prinzipien nicht verwendet werden. Aber der Umstieg auf Agilität hat sehr viele Fallstricke und kann gehörig nach hinten losgehen. Starten Sie klein und übernehmen Sie sich nicht, akzeptieren Sie Fehler, lernen Sie aus Ihnen, lassen Sie Transparenz zu und bringen Sie Geduld mit.

Steigen wir in das technische Wissen ein. Es sind zwei größere Kapitel. Einmal die technischen Grundbegriffe. Hier erläutern wir Themen wie den Hardwareaufbau, wie Hardware mit Software zusammenhängt, wie Software erstellt wird, wie das Internet funktioniert und so weiter. Im nächsten Kapitel „Umsetzung einer Idee“ bauen wir auf dem gelernten auf und versuchen uns an einer theoretischen Umsetzung einer Applikation.

 _q_

_Umsetzung einer Idee_

Wir haben jetzt wichtige Kapitel der Software Entwicklung behandelt. Wie nutzt man dies jetzt um eine Idee umzusetzen. Lassen Sie uns das einmal zwei Punkte theoretisch durchspielen, das Requirements Engineering und die Software Architektur. Eine detailiertere Beschreibung der Software Entwicklung würde den Rahmen dieses Buches sprengen. 

Die Idee nehmen wir einfach eine App mit der man mit zufälligen anderen Menschen chatten kann. Das gibt es schon, ist nichts Außergewöhnliches, aber das ist hier vollkommen irrelevant.

Idee konkretisieren

Unsere Vision könnte sein: „Wir wollen die Verständigung zwischen Menschen verbessern, indem wir zufällig ausgewählte Menschen anonym miteinander chatten lassen“. Macht jetzt vielleicht nicht viel her, ist aber auch nicht der Fokus in diesem Buch. Nächster Schritt wären die Funktionen. Sie können sich selber einmal grobe Punkte überlegen die sie umsetzen würden. 

Hier ein paar Ideen:

- Login über Facebook und andere bekannte Provider.
- Wenn man die App öffnet kommt man direkt auf ein Menü wo man 3 Dinge auswählen kann:
-- Chatten
-- Verlauf
-- Themen
-- Profil
- Wenn man auf Chatten klickt startet eine Suche die einen mit einem anderen Chatpartner der noch frei ist und der auch auf Chatten geklickt hat verbindet
- Wenn die Suche erfolgreich ist kann ist oben der Chatverlauf und unten die Textbox
- Man kann Nachtrichten schicken und den Chatkontakt als positiv oder negativ markieren. 
- Wird der Chat negativ markiert dann wird der Chatkontakt geblockt.
- Wenn beide den Chatkontakt mit positiv markieren sind die beiden Chatkontakte verbunden und sie können weiterchatten bis einer auf negativ drückt.
1. 
- Wenn man auf Verlauf klickt werden einem die letzten Chats angezeigt. Man kann mit Chatkontakten die von beiden als positiv markiert sind weiterchatten.
- Unter Themen kann man stichpunktartig Themen aufschreiben für die man sich interessiert.
- Unter Profil kann man seine Sprachen festlegen in denen man chatten möchte.

Gehen wir damit einmal ins Rennen. Nächster Schritt wäre jetzt den MVP zu klären und welche Anforderungen man zum Starten der App wirklich benötigt. Gehen wir davon aus das wir nur den Punkt Chatten benötigen. Man klickt darauf und wird mit einer zufälligen anderen Person die auch gerade auf Chatten geklickt hat und kann mit diesem Textnachrichten austauschen.

Diese kann man jetzt noch in Userstories aufteilen:

- Der Nutzer öffnet die App und es erscheint die Startseite mit den Button Chatten damit er die App überhautp nutzen kann
- Der Nutzer klickt auf Chatten um sich für zufälliges Chatten anzumelden. Es wird ein Ladebalken angezeigt der visualisiert, dass gerade verfügbare Chats gesucht werden. Die Chats werden priorisiert nach gleichen Ländern gesucht.
- Wenn ein Chat gefunden wurde, wird unten ein Texteingabefeld angezeigt wo der Nutzer Text eingeben kann um auch Nachrichten verschicken zu können
- ….

Nicht funktionale Anforderungen sollte man natürlich nicht vergessen:

- Die App sollte auf so vielen mobilen Geräten wie möglich funktionieren. Dazu gehörten Smartphones und Tablets
- Die App sollte von sehr vielen Nutzern gleichzeitig verwendet werden können.
- Chats sollten priorisiert nach Sprachen automatisch gesucht werden.

Wir wollen das jetzt nicht weiter vertiefen. Die grundsätzliche Idee dahinter sollte klar sein.

Software Architektur

Im nächsten Schritt sollte man sich Gedanken machen wie man dieses Projekt technisch umsetzt. Hier kommt die Software Architektur ins Spiel die sich mit den Fragen beschäftigt. Die App soll übergreifend auf den gängigen Smartphones laufen als Android und IPhone Handys und Tablets. Versuchen wir mal die Gedanken eines Software Architekten einzufangen:

- Die App zielt darauf ab, auf vielen Endgeräten weltweit gleichzeitig performant zu funktionieren. Dazu kommt, dass im Idealfall in kurzer Zeit die Anzahl der Nutzer sehr stark ansteigt. Deswegen muss die Software sehr gut skalieren. 

- Da die App von einer Kommunikation zwischen Usern lebt, ist es sinnvoll die Anwendung in zwei verschiedene Ebenen aufzuteilen. Die App auf der einen Seite, also das Programm welches beim Kunden auf dem Endgerät läuft und einen (oder mehrere Services) welches die Nutzer dann am Schluss zufällig zusammenbringt. Das nennt man die Aufteilung in Frontend und Backend. Das Frontend läuft auf dem Endgerät und kommuniziert mit dem Backend welche auf zentralen im Internet verfügbaren Servern läuft.
1. 

1. 
- Nachdem wir eine große Anzahl von Nutzern gleichzeitig verarbeiten können müssen, wäre es wichtig, dass wir das Backend auf mehreren Servern laufen lassen können, da ein Server immer begrenzte Ressourcen hat. Das führt aber dazu, dass wenn zwei Nutzer die sich auf unterschiedlichen Backends verbinden erst mal nicht sehen würden.
1. 

- Deswegen kann entweder eine Kommunikation zwischen Backends aufbauen, eine gemeinsame Datenbank verwenden oder die gesamte Funktionalität des Backends in kleine unabhängige Services aufteilen. Z.B. Profil Anfragen verarbeitet ein Service auf einem Server und das Finden von geeigneten Random Chats ein anderer. Das würde man dann Microservice Architektur nennen. Macht vor allem dann Sinn wenn man mit mehreren Teams arbeitet. 

        

- Das Backend kann man mit vielen verschiedenen Programmiersprachen entwickeln. Das kommt etwas auf den Anwendungsfall an. Diese Entscheidung hängt daran welche Programmiersprachen die Entwickler beherrschen. Hier kann man Nodejs, GO, Java oder andere Sprachen nutzen.
1. 
- Eine Datenbank wird benötigt da wir, falls ein Backend neu gestartet werden muss, den Zustand nicht verliert z.B. die aktuell angemeldeten Benutzer. Außerdem kann damit jedes Backend auf die aktuelle Liste aller Benutzer zugreifen. Hier muss aber auch wieder eine Datenbank gewählt werden die skalierbar ist, sonst hat man seine Skalierung verloren. Dafür eignen sich gut dokumentbasierte Datenbanken, z.B. MongoDB
1. 

1. 
- Der Backend-Service muss global im Internet einfach erreichbar sein und muss auch mit großen Anfragen gut zurechtkommen. Deswegen sollte der Service in einer Cloud installiert werden. In Frage kommen dafür Heroku, AWS, Azure, Google oder andere. Die Entscheidung hängt wieder von vielen Faktoren wie der Erfahrung der Entwickler im Team, aber auch die anfallenden Kosten. Dann muss man sich entscheiden welche der Angebote innerhalb der Cloud für einen passen. Benötigt man einen Kubernetes Cluster oder reicht eine normale Webapp. Auch die Datenbank muss in der Cloud als Service verfügbar sein. In Azure z.B. könnte man statt MongoDB die CosmosDB nehmen.

1. 

- Ein Großteil des Traffics dürfte durch den Austausch von Nachrichten entstehen. In der aktuellen Überlegung müsste eine Nachricht von dem Endgerät an das Backend geschickt werden. Das Backend muss die Nachricht in die Datenbank schreiben, da wir nicht sicher sein können, dass die Apps mit den gleichen Backends verbunden sind. Somit würde man viel Last auf der Datenbank erzeugen. Das könnte man abfedern in dem man die Funktionalität die Nachrichten zu versenden und empfangen nicht über das Backend laufen lässt, sondern dafür einen eigenen Service nutzt, der nur für Daten verschicken und empfangen genutzt werden kann. Dieser Service sollte sehr ausfallsicher sein. Hier kann man z.B. Nats.io, RabbitMQ oder Kafka nutzen, sogenannte Message Broker. Das sind Services die extra dafür entwickelt wurden eine große Anzahl von Nachrichten skalierbar zu versenden. Damit könnte man das Backend für die Registrierung von Nutzern nutzen sowie das zufällige zusammenschließen von Nutzern. Die Chats an sich laufen dann aber über den Message Broker.Damit könnte es sein, dass wir das Backend nicht skalieren müssen. Aber natürlich sollte das trotzdem in Zukunft möglich sein, somit sollte man eine skalierbare Datenbank wählen.
1. 
1. 
- 
1. 
- Im Frontend muss man mehrere Endgeräte unterstützen. IOS und Android sind zwei unterschiedliche Betriebssysteme und nicht kompatibel so dass man das Frontend zweimal programmieren muss mit unterschiedlichen Technologien, Swift für IOS und Java für Android. D.h. die Entwickler müssen auch beide Technologien kennen. Um das zu vermeiden kann man auf Technologien setzen wo man eine Codebasis programmiert diese aber auf beiden Geräten laufen, die haben aber öfters Einschränkungen und können nicht alle Funktionen eines Handys voll ausnutzen. Z.B. könnte man rein browserbasiert arbeiten mit HTML, CSS, Javascript. Das hätte den Vorteil dass man die Applikation ohne Installation auf allen Endgeräten nutzen kann. Der Nachteil ist, dass man Handyfunktionen wie Sensoren oder GPS nicht so einfach nutzen kann. Geeignet wären Technologien wie Flutter, Cordova oder React Native für das Frontend.

Diese Entscheidung die der Software Architekt zusammen mit dem Team fällt, hängt an den Anforderungen, den Möglichkeiten der Technologien und der Erfahrung der Entwickler ab. Wenn ein Team schon viel Erfahrung mit Webtechnologien hat, warum soll man diese Erfahrung nicht einsetzen, auch wenn man vielleicht ein paar Nachteile in Kauf nehmen muss, wie der Zugriff auf rudimentäre Handyfunktionen. Wenn die Applikation sich über mehrere Teams verteilt, spielt die Organisation in der Architektur auch eine große Rolle wie wir in dem Kapitel Conways Law gesehen haben. Bei mehreren Teams kann es sein, dass die Microservice Architektur mehr Sinn macht um die Abhängigkeiten der Teams voneinander zu minimieren.

Lassen wir es dabei. Kurz zusammengefasst:

- Aufteilung der Applikation in Frontend und Backend. 
- Das Backend wird mit GO implementiert. Es implementiert Profile und findet geeignete Chatpartner. Wenn ein Chatpartner gefunden ist bekommen diese ein zufälliges Topic zugewiesen mit dem sie über den Message Broker Nachrichten austauschen können. Die Chatverläufe werden nur lokal auf der App gespeichert.
- Datenbanktechnologie wird auf eine document based skalierbare Datebank gesetzt
- Das Frontend wird mit Flutter implementiert um die App nur einmal entwickeln zu müssen

Es gibt immer viele Möglichkeiten eine Software Architektur aufzubauen. Das ist ein kreativer Prozess und lässt sich schlecht standardisieren. Die Patterns sind eine Möglichkeit für bestimmte Probleme ähnliche Lösungen die funktionieren umzusetzen. 

Fazit

Lassen wir es hierbei bewenden. Falls Sie wirklich es bis hierhin geschafft haben, sollten Sie jetzt einen Eindruck von der Software Entwicklung und seiner Komplexität bekommen haben. Vielleicht verstehen Sie bei Ihren Entwicklungsteams etwas mehr und können sogar etwas mitreden, dann hätte dieses Buch seinen Zweck erfüllt. Aber es liegt an Ihnen nicht stehen zu bleiben, sich weiter fortzubilden und am Ball zu bleiben. Das ist Software Entwicklung. Viele der hier genannten Technologien sind in 5 Jahren obsolet. Wenn Sie es in der Macht haben, versuchen Sie sich vor allem mit Agilität auseinander zu setzen und einen Rahmen für Ihre Entwickler in erschaffen, dass diese optimal arbeiten können. Vertrauen Sie Ihren Entwicklern und Product Ownern zu, dass diese gute Lösungen umsetzen. Dann können Sie das Thema Digitalisierung meistern und Ihr Unternehmen auf das nächste Level heben.

Multithreading

1. 
1. Hier wird das ganze Software Entwickeln schon richtig komplex. Nachdem das Betriebssystem der Manager der Hardware ist, muss ein Prozess dieses um Rechenleistung bitten. Das Betriebssystem weist den Programmen Prozessorleistung zu, indem es ca. alle 20ms alle Programme unterbricht das Betriebssystem alle Prozesse durchsucht wer Rechenleistung beantragt hat und entscheidet welches als nächstes die CPU nutzen darf. Somit gaukelt man den Benutzer vor, dass er mehrere Programme gleichzeitig nutzen kann. Das Betriebssystem ruft jetzt nicht direkt die Prozesse auf und sagt: „Prozess A bitte jetzt rechnen“ sondern in einem Prozess laufen mehrere sogenannte Threads. Ein Prozess kann in seinem Programmcode einen Thread starten und in diesem Programmcode laden. Der Thread kümmert sich dann um die Ausführung.
1. 
1. Warum benötigt man so etwas? Sagen wir einfach Sie hätten Software das größere Reportings für ein Unternehmen generieren kann. Das Erstellen der Reportings ist komplex und dauert seine Zeit. Der Nutzer drückt auf den Button, der die Berechnung anstößt. Wenn das Erstellen des Reportings nicht in einen Thread ausgelagert wird, dann friert die Oberfläche ein bis den Report erstellt ist und Sie können nicht weiterarbeiten. Wenn das Generieren des Reports aber in einem extra Thread ausgelagert ist, können Sie in der Oberfläche weiterarbeiten, während ein Ladebalken in der Statusleiste anzeigt, wann der Report fertig ist. Und mit Multicore CPUs hat das Thema noch einmal mehr an Bedeutung gewonnen. Wenn man eine 8 Core CPU hat, können 8 Threads wirklich parallel arbeiten und man kann die Applikation in der Ausführung beschleunigen.

1. Multithreading hat viele Vorzüge aber ist für den Entwickler oft problematisch. Nachdem das OS entscheidet welcher Thread wann ausgeführt wird und das nicht in der Hand des Entwicklers liegt kann es verrückte Effekte und Fehler geben, die zufällig passieren und die man nicht einfach nachstellen kann. Eines der Effekte sind sogenannte Race Conditions, kurz gesagt bedeutet das, dass man ein und dasselbe Programm startet und selten unterschiedliche Ergebnisse bekommt.
1. 
1. Ein Beispiel für eine Race Condition:
1. 
1. Wir wollen ein Integer mit Wert 1 um eins erhöht. Man sagt dazu auch Inkrementieren. Aber der gleiche Code wird in zwei Thread gleichzeitig ausgeführt. Obwohl das in den meisten Programmiersprachen nur eine Zeile Code ist (i++) aber wir haben ja gelernt, dass das in CPU Befehle übersetzt wird. Somit ist ein einfacher inkrement mit dem ++ Operator in Wirklichkeit 3 CPU Befehle.
1. 
1. Lade aus Speicheradresse x in die CPU
1. Erhöhe den Wert um eins
1. Speichere den neuen Wert wieder in x
1. 
1. In der nächsten Abbildung wird der Code ausgeführt wie man das erwarten würde. Zwei Threads laufen erhöhen den Wert jeweils um 1 und es kommt der Wert 3 heraus.
1. 

Abbildung 23 Keine Race Condition

1. In der nächsten Abbildung unterbricht das OS den Thread 1 nachdem der Wert geladen wurde, wechselt zu Thread 2 der jetzt auch den Wert 1 aus dem Speicher liest. Der Wert wird auf 2 erhöht und dann gespeichert. Hier ist der Thread fertig, das OS lässt Thread 1 wieder laufen setzt den Zustand innerhalb der CPU wieder so wie er bei Thread 1 war und Inkrementiert den Wert der geladen wurde, nämlich die 1. Somit haben wir also Endergebnis die zwei obwohl wir zweimal den Wert inkrementiert haben. 
1. 

Abbildung 24 Unerwartetes Ergebnis durch Race Condition

Das nennt man eine Race Condition und man muss sehr stark als Entwickler aufpassen, dass man solche Effekte von vorneherein vermeidet. Diese Fehler nachzuvollziehen bzw. zu debuggen ist sehr schwer. Es gibt auch Möglichkeiten solche Probleme zu verhindern aber wir gehen da nicht tiefer darauf ein.

